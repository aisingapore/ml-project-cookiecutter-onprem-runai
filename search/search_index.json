{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"End-to-end Project Template (On-premise Run:ai)","text":"<p>Customised for <code>{{cookiecutter.project_name}}</code>.</p> <p>Project Description: {{cookiecutter.description}}</p> <p>This template that is also accompanied with an end-to-end guide was generated and customised using the following <code>cookiecutter</code> template:</p> <p>https://github.com/aisingapore/ml-project-cookiecutter-onprem-runai</p> <p>This <code>mkdocs</code> site is for serving the contents of the end-to-end guide in a more readable manner, as opposed to plain Markdown views. The contents of this guide have been customised according to the inputs provided upon generation of this repository through the usage of the <code>cookiecutter</code> CLI, following instructions detailed here .</p> <p>Inputs provided to <code>cookiecutter</code> for the generation of this template:</p> <ul> <li><code>project_name</code>: {{cookiecutter.project_name}}</li> <li><code>description</code>: {{cookiecutter.description}}</li> <li><code>repo_name</code>: {{cookiecutter.repo_name}}</li> <li><code>src_package_name</code>: {{cookiecutter.src_package_name}}</li> <li><code>src_package_name_short</code>: {{cookiecutter.src_package_name_short}}</li> <li><code>harbor_registry_project_path</code>: {{cookiecutter.harbor_registry_project_path}}</li> <li><code>author_name</code>: {{cookiecutter.author_name}}</li> </ul>"},{"location":"#overview-for-user-guide","title":"Overview For User Guide","text":"<ol> <li>Prerequisites</li> <li>Preface</li> <li>MLOps Components &amp; Platform</li> <li>Developer Workspace</li> <li>Virtual Environment</li> <li>Data Storage &amp; Versioning</li> <li>Job Orchestration</li> <li>Deployment</li> <li>Batch Inferencing</li> <li>Continuous Integration &amp; Deployment</li> <li>Documentation</li> </ol>"},{"location":"#directory-tree","title":"Directory Tree","text":"<pre><code>{{cookiecutter.repo_name}}\n\u251c\u2500\u2500 aisg-context        &lt;- Folders containing files and assets relevant\n\u2502   \u2502                      for works within the context of AISG's\n\u2502   \u2502                      development environments.\n\u2502   \u2514\u2500\u2500 guide-site      &lt;- Files relevant for spinning up the `mkdocs`\n\u2502                          site to view the end-to-end guide.\n\u251c\u2500\u2500 conf                &lt;- Configuration files associated with the\n\u2502                          various pipelines as well as for logging.\n\u251c\u2500\u2500 data                &lt;- Folder to contain any data for the various\n\u2502                          pipelines. Ignored by Git except its\n\u2502                          `.gitkeep` file.\n\u251c\u2500\u2500 docker              &lt;- Dockerfiles associated with the various\n\u2502                          stages of the pipeline.\n\u251c\u2500\u2500 docs                &lt;- A default Sphinx project; see sphinx-doc.org\n\u2502                          for details.\n\u251c\u2500\u2500 models              &lt;- Directory for trained and serialised models.\n\u251c\u2500\u2500 notebooks           &lt;- Jupyter notebooks. Suggested naming\n\u2502                          convention would be number (for ordering),\n\u2502                          the creator's initials, and a short `-`\n\u2502                          delimited description, e.g.\n\u2502                          `1.0-jqp-initial-data-exploration`.\n\u251c\u2500\u2500 src                 &lt;- Directory containing the source code and\n|   |                       packages for the project repository.\n\u2502   \u251c\u2500\u2500 {{cookiecutter.src_package_name}}\n\u2502   \u2502   ^- Package containing modules for all pipelines except\n\u2502   \u2502      deployment of API server.\n\u2502   \u251c\u2500\u2500 {{cookiecutter.src_package_name}}_fastapi\n\u2502   \u2502   ^- Package for deploying the predictive models within a FastAPI\n\u2502   \u2502      server.\n\u2502   \u2514\u2500\u2500 tests           &lt;- Directory containing tests for the\n\u2502                          repository's packages.\n\u251c\u2500\u2500 .dockerignore       &lt;- File for specifying files or directories\n\u2502                          to be ignored by Docker contexts.\n\u251c\u2500\u2500 .gitignore          &lt;- File for specifying files or directories\n\u2502                          to be ignored by Git.\n\u251c\u2500\u2500 .gitlab-ci.yml      &lt;- AML file for configuring GitLab CI/CD\n\u2502                          pipelines.\n\u251c\u2500\u2500 .pylintrc           &lt;- Configurations for `pylint`.\n\u251c\u2500\u2500 {{cookiecutter.repo_name}}-conda-env.yaml\n\u2502   ^- The `conda` environment file for reproducing\n\u2502      the project's development environment.\n\u2514\u2500\u2500 README.md           &lt;- The top-level README containing the basic\n                           guide for using the repository.\n</code></pre>"},{"location":"guide-for-user/01-prerequisites/","title":"Prerequisites","text":""},{"location":"guide-for-user/01-prerequisites/#software-tooling-prerequisites","title":"Software &amp; Tooling Prerequisites","text":"<p>Aside from an internet connection, you would need the following to follow through with the guide:</p> <ul> <li>NUS Staff/Student account.</li> <li>Azure account provisioned by AI Singapore.</li> <li>PC with the following installed:<ul> <li>If your machine is with a Windows OS, use   PowerShell   instead of the default Command (<code>cmd.exe</code>) shell. Best if you   resort to   Windows Terminal.</li> <li>Pulse Secure<ul> <li>Refer to NUS IT eGuides   for installation guides.</li> </ul> </li> <li>Web browser</li> <li>Terminal</li> <li>Git</li> <li>Rancher Desktop or   Docker Engine:   Client-server application for containerising applications as well   as interacting with the Docker daemon.<ul> <li>For Linux users, you may install the Docker Engine (Docker daemon)   directly.</li> <li>For Windows or macOS users, the Docker daemon can be installed   through Rancher Desktop.</li> </ul> </li> <li>miniconda:   for Python virtual environment management.</li> <li><code>kubectl</code>:   CLI for Kubernetes.</li> <li>AWS CLI: CLI for AWS services, but we will specifically be using it   for interacting with the AI Singapore's Elastic Cloud Storage   (ECS) service through the S3 protocol.<ul> <li>You may choose to just use   <code>boto3</code>,   the Python SDK for AWS instead, to interact with the ECS   service within a Python environment. However, this does   not fall under the scope of this guide.</li> </ul> </li> <li>(Optional) <code>helm</code>:   CLI for Kubernetes' package manager.</li> </ul> </li> <li>Access to a project on AI Singapore's Run:ai cluster.   See here for more information.</li> <li>Credentials for AI Singapore's Elastic Cloud Storage (ECS) service.   See here for more information.</li> <li>Credentials for AI Singapore's Harbor registry.   See here for more information.</li> <li>Credentials for an MLflow Tracking server.   See here for more information.</li> </ul> <p>Note</p> <p>If you do not have any of the required credentials, please verify with or notify the MLOps team at <code>mlops@aisingapore.org</code>.</p> <p>Info</p> <p>Wherever relevant, you can toggle between the different commands that need to be executed for either Linux/macOS or the Windows environment (PowerShell). See below for an example:</p> Linux/macOSWindows PowerShell <pre><code># Get a list of files/folders in current directory\n$ ls -la\n</code></pre> <pre><code># Get a list of files/folders in current directory\n$ Get-ChildItem . -Force\n</code></pre> <p>Warning</p> <p>If you are on Windows OS, you would need to ensure that the files you've cloned or written on your machine be with <code>LF</code> line endings. Otherwise, issues would arise when Docker containers are being built or run. See here on how to configure consistent line endings for a whole folder or workspace using VSCode.</p>"},{"location":"guide-for-user/01-prerequisites/#tips-and-tricks","title":"Tips and Tricks","text":"<ul> <li>If you're using Rancher Desktop, you might encounter issues with    regards to the lack of CPU and memory.<ul> <li>For Mac/Linux users, from the main window, click on the gear    button on the top right.   Then, proceed to the Virtual Machines section and increase your    CPU and memory resources directly.</li> <li>For Windows users, create a <code>.wslconfig</code> file user <code>%UserProfile%</code>    with the following content:   <pre><code>[wsl2]\nmemory=8GB\n</code></pre>   Change the amount of memory to something you're comfortable with    giving up to build Docker images.</li> </ul> </li> <li>For Windows users, if you have both Rancher and Docker Desktop    installed, you may need to disable the networking tunnel:<ul> <li>From the gear button on the top right, go to the WSL section    under the Network tab. From there, uncheck the <code>Enable networking    tunnel</code>.</li> </ul> </li> </ul>"},{"location":"guide-for-user/02-preface/","title":"Preface","text":""},{"location":"guide-for-user/02-preface/#repository-setup","title":"Repository Setup","text":"<p>This repository provides an end-to-end template for AI Singapore's AI engineers to onboard their AI projects. Instructions for generating this template is detailed in the <code>cookiecutter</code> template's repository's <code>README.md</code>.</p> <p>While this repository provides users with a set of boilerplates, here you are also presented with a linear guide on how to use them. The boilerplates are rendered and customised when you generated this repository using <code>cookiecutter</code>.</p> <p>Info</p> <p>You can begin by following along the guide as it brings you through a simple problem statement and once you've grasp what this template has to offer, you can deviate from it as much as you wish and customise it to your needs.</p> <p>Since we will be making use of this repository in multiple environments, ensure that this repository is pushed to a remote. Most probably you will be resorting to AI Singapore's GitLab instance as the remote. Refer to here on creating a blank remote repository (or project in GitLab's term). After creating the remote repository, retrieve the remote URL and push the local repository to remote:</p> <pre><code>$ git init\n$ git remote add origin &lt;REMOTE_URL&gt;\n$ git add .\n$ git config user.email \"&lt;YOUR_AISG_EMAIL&gt;\"\n$ git config user.name \"&lt;YOUR_NAME&gt;\"\n$ git commit -m \"Initial commit.\"\n$ git push -u origin main\n</code></pre> <p>Go to this section for more information on interacting with the on-premise GitLab instance.</p>"},{"location":"guide-for-user/02-preface/#guides-problem-statement","title":"Guide's Problem Statement","text":"<p>For this guide, we will work towards building a neural network that is able to classify handwritten digits, widely known as the MNIST use-case. The model is then to be deployed through a REST API and used for batch inferencing as well. The raw dataset to be used is obtainable through a Google Cloud Storage bucket; instructions for downloading the data into your development environment are detailed under \"Data Storage &amp; Versioning\", to be referred to later on.</p> <p>Info</p> <p>License: Yann LeCun and Corinna Cortes hold the copyright of MNIST dataset. MNIST dataset is made available under the terms of the Creative Commons Attribution-Share Alike 3.0 license.</p>"},{"location":"guide-for-user/03-mlops-components-platform/","title":"MLOps Components &amp; Platform","text":"<p>The aim for this section is to deliver the following:</p> <ul> <li>An overview of the various MLOps components that we will be   interacting with for the rest of the guide, as well as an overview for   each of the components.</li> <li>A summary of the service(s) and tool(s) of choice for some components,   and an access quickstart for each of them.</li> </ul>"},{"location":"guide-for-user/03-mlops-components-platform/#nus-vpn","title":"NUS VPN","text":"<p>Your credentials for your NUS Staff/Student account is needed to login to NUS' VPN for access to the following:</p> <ul> <li>AI Singapore's GitLab instance</li> <li>AI Singapore's Kubernetes clusters</li> <li>AI Singapore's Run:ai platform</li> <li>AI Singapore's Harbor registry</li> <li>AI Singapore's Elastic Cloud Storage (ECS)</li> <li>Your project's on-premise MLflow Tracking server</li> <li>Other miscellaneous NUS resources</li> </ul>"},{"location":"guide-for-user/03-mlops-components-platform/#kubernetes","title":"Kubernetes","text":"<p>Before we dive into the different MLOps components that you will be interacting with in the context of this guide, we have to first introduce Kubernetes as the underlying orchestration tool to execute pipelines and manage containerised applications and environments.</p> <p>From the Kubernetes site:</p> <p>Kubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications. It groups containers that make up an application into logical units for easy management and discovery.</p> <p>A number of services and applications that you will be interacting with (or deploying) are deployed (to be deployed) within a Kubernetes cluster. Some of the MLOps components which the Kubernetes cluster(s) will be relevant for are:</p> <ul> <li>Developer Workspace</li> <li>Data Experimentation</li> <li>Model Training &amp; Evaluation</li> <li>Experiment &amp; Pipeline Tracking</li> <li>Model Serving</li> </ul> <p>These components will be further elaborated in the upcoming sections.</p> Reference Link(s) <ul> <li>IBM - What is Kubernetes?</li> </ul>"},{"location":"guide-for-user/03-mlops-components-platform/#rancher","title":"Rancher","text":"<p>Upon one's assignment to a project, any relevant clusters that one has access to would be viewable on AI Singapore's Rancher dashboard.</p> <p></p> <p>Rancher is a Kubernetes management platform that provides cluster administrators or users to manage Kubernetes clusters or facilitate Kubernetes workflows. To login, use your Azure account i.e. the same set of credentials that you use for your GitLab account.</p> <p>Note</p> <p>If you do not have rightful access to a cluster, please notify the MLOps team at <code>mlops@aisingapore.org</code>.</p>"},{"location":"guide-for-user/03-mlops-components-platform/#kubernetes-vs-rancher-vs-runai","title":"Kubernetes VS Rancher VS Run:ai","text":"<p>One might be confused as to how each of the aforementioned tools and platforms differ from each other. To put it simply, Kubernetes lies underneath the Rancher and Run:ai platform/interface. Rancher and Run:ai are abstraction layers on top of Kubernetes; they both essentially communicate with the Kubernetes API server to carry out actions or orchestrate workloads through each of their own interface.</p> <p>Developers can use Rancher's interface or Run:ai's interface/CLI to spin up workspaces, jobs or deploy applications. However, the latter can better serve machine learning engineers in carrying out their machine learning workflows as that was the intended usage of the platform. Besides, Run:ai's unique selling point is its better utilisation of GPU resources (through fractionalisation and other features) so when it comes to workloads that require GPU, like model training and evaluation, the usage of Run:ai is recommended. Also, on the surface, it is easier for one to spin up developer workspaces on Run:ai.</p> Reference Link(s) <ul> <li>Rancher Docs - Rancher Server and Components</li> <li>Run:ai Docs - System Components</li> </ul>"},{"location":"guide-for-user/03-mlops-components-platform/#mlops-components","title":"MLOps Components","text":"<p>The diagram below showcases the some of the components that this guide will cover as well as how each of them relate to each other.</p> <p></p> <p>Note</p> <p>Click on the image above for an interactive view of the diagram. You may interact with the layers to view the components in a sequential manner.</p>"},{"location":"guide-for-user/03-mlops-components-platform/#developer-workspace","title":"Developer Workspace","text":"<p>Developers begin by having the client (laptop/VM) to be authenticated by whichever platform they have been provided access to.</p> <ul> <li>Developers with access to Google Cloud projects would have to   authenticate through the <code>gcloud</code> CLI which allows them to access the   Google Kubernetes Engine (GKE) cluster, which in turn would allow them   access to the default orchestration platform, Polyaxon.</li> <li>Developers with access to AI Singapore\u2019s on-premise clusters would   have to authenticate through the default orchestration platform that   runs on top of the on-premise Kubernetes clusters, Run:ai.   This is done through Run:ai\u2019s CLI.</li> </ul> <p>Following authentication, developers can make use of templates provided by the MLOps team to spin up developer workspaces (VSCode server, JupyterLab, etc.) on the respective platforms. Within these developer workspaces, developers can work on their codebase, execute light workloads, and carry out other steps of the end-to-end machine learning workflow.</p> <p>A typical machine learning or AI project would require the team to carry out exploratory data analysis (EDA) on whatever domain-specific data is in question. This work is expected to be carried out within the development workspace with the assistance of virtual environment managers.</p> Reference Link(s) <ul> <li>Run:ai Docs - Workspaces Introduction</li> </ul>"},{"location":"guide-for-user/03-mlops-components-platform/#version-control","title":"Version Control","text":"<p>Within a developer workspace and environment, developers can interact (pull, push, etc.) with AI Singapore\u2019s GitLab instance, which serves as the organisation\u2019s default version control (Git) remote server.</p> Reference Link(s) <ul> <li>Atlassian Tutorials - What is Git?</li> </ul>"},{"location":"guide-for-user/03-mlops-components-platform/#continuous-x","title":"Continuous X","text":"<p>GitLab also serves as a DevOps platform where the Continuous X of things (Continuous Integration, Continuous Delivery, etc.) can be implemented and automated. This is done through GitLab CI/CD. Interactions made with repositories on GitLab can be made to trigger CI/CD workflows. The purpose of such workflows are to facilitate the development lifecycle and streamline the process of delivering quality codebase.</p> <ul> <li>The workflows at the very least should include unit and integration   testing where the codebase is subjected to tests and linting tools   to ensure that best practices and conventions are adhered to by   contributors from the project team. This is known as   Continuous Integration (CI).</li> <li>Another important aspect is Static Application Security Testing   (SAST) where application security tools are utilised to identify   any vulnerabilities that exist within the codebase.</li> <li>GitLab CI/CD can also invoke interactions with other MLOps   components such as submitting jobs   (model training, data processing, etc.) to the   aforementioned orchestration platforms or even deploy applications.   This fulfils the Continuous Delivery (CD) and   Continuous Training (CT) portion.</li> </ul> Reference Link(s) <ul> <li>ml-ops.org - Continuous X</li> <li>Google Cloud - MLOps: Continuous delivery and automation pipelines in machine learning</li> <li>GitLab - What is DevOps?</li> </ul>"},{"location":"guide-for-user/03-mlops-components-platform/#container-image-registry","title":"Container Image Registry","text":"<p>AI Singapore has a strong emphasis on containerising pipelines for the purpose of reproducibility and ease of delivery. Images built through CI/CD workflows or manual builds can be pushed to container image registries, be it Google Cloud\u2019s Artifact Registry or AI Singapore\u2019s on-premise Harbor registry.</p> <p></p> <p>Harbor Registry</p> Reference Link(s) <ul> <li>Red Hat - What is a container registry?</li> </ul>"},{"location":"guide-for-user/03-mlops-components-platform/#data-preparation","title":"Data Preparation","text":"<p>Following the EDA phase, the project team would map out and work on data processing and preparation pipelines. These pipelines would first be developed with manual invocation in mind but a team can strive towards automating the processes where the pipelines can be triggered by the CI/CD workflows that they would have defined.</p> <p>As the quality of data to be used for training the models is important, components like data preparation can be prefaced with data validation, where checks are done to examine the data\u2019s adherence to conventions and standards set by the stakeholders of the project.</p>"},{"location":"guide-for-user/03-mlops-components-platform/#model-training-evaluation","title":"Model Training &amp; Evaluation","text":"<p>Once the project team is more familiar with the domain-specific data and data preparation pipelines have been laid, they can look into model training and evaluation.</p> <p>When working towards a base model or a model that can be settled as the Minimum Viable Model (MVM), a lot of experimentations would have to be done as part of the model training process. Part of such experiments includes parameter tuning where a search space is iterated through to find the best set of configurations that optimises the model\u2019s performance or objectives. Tools like Optuna can greatly assist in facilitating such workflows.</p>"},{"location":"guide-for-user/03-mlops-components-platform/#experiment-pipeline-tracking","title":"Experiment &amp; Pipeline Tracking","text":"<p>As there would be a myriad of experiments to be carried out, there is a need for the configurations, results, artefacts, and any other relevant metadata of every experiment to be logged and persisted. Purpose of tracking such information would allow for easy comparison of models\u2019 performances and if there is a need to reproduce experiments, relevant information can be referred back. With the right information, metadata and utilisation of containers for reproducible workflows, pipelines can be tracked as well. Carrying these out would provide a team with a model registry of sorts where experiments with tagged models can be referred to when they are to be deployed and served.</p> <p>A tool with relevant features would be MLflow.</p> Reference Link(s) <ul> <li>Databricks Blog - Introducing MLflow: an Open Source Machine Learning Platform</li> </ul>"},{"location":"guide-for-user/03-mlops-components-platform/#model-serving","title":"Model Serving","text":"<p>With the models that have been trained, applications that allow for end-users to interact with the model can be deployed on test environments. Deployment of models can be and are conventionally done by using API frameworks. However, not all problem statements require such frameworks and scripts for executing batch inference might suffice in some cases.</p> <p>One of the popular Python frameworks for building APIs is FastAPI. It is easy to pick up and has many useful out-of-the-box features.</p> Reference Link(s) <ul> <li>Ubuntu Blog - A guide to ML model serving</li> </ul>"},{"location":"guide-for-user/03-mlops-components-platform/#gitlab","title":"GitLab","text":"<p>We at AI Singapore host our own GitLab server:</p> <p>https://gitlab.aisingapore.net</p> <p>You should be provided with a set of credentials during onboarding for access to the server.</p> <p>In order to interact with remote Git repositories situated on AI Singapore's GitLab instance (clone, push, fetch, etc.) outside of NUS' network or GCP (regions <code>asia-southeast1</code> and <code>us-central1</code>), you would need to login to NUS' VPN.</p>"},{"location":"guide-for-user/03-mlops-components-platform/#push-pull-with-https-vs-ssh","title":"Push &amp; Pull with HTTPS VS SSH","text":"<p>The usage of either the HTTPS or SSH protocol for communicating with the GitLab server depends on the environment in question. If an environment is made accessible by multiple developers, then HTTPS-based access where passwords are prompted for would be better fitting. SSH-based access would be more fitting for clients that are more isolated like a single Linux user or local machines made accessible by a single owner.</p> <p>If you would like to configure SSH access for accessing AI Singapore's GitLab instance, you can add the following lines to your SSH configuration file (<code>~/.ssh/config</code>):</p> <pre><code>Host gitlab.aisingapore.net\n    Port 2222\n    IdentityFile ~/.ssh/&lt;PATH_TO_PRIVATE_KEY&gt;\n</code></pre> Reference Link(s) <ul> <li>GitLab Docs - Use SSH keys to communicate with GitLab</li> </ul>"},{"location":"guide-for-user/03-mlops-components-platform/#runai","title":"Run:ai","text":"<p>Run:ai is an enterprise orchestration and cluster management platform that works as an abstraction layer on top of AI Singapore's hybrid infrastructure to maximise the usage of such resources. The platform utilises Kubernetes in the backend. Orchestration platforms such as Run:ai allows end-users to easily spin up workloads, execute jobs, set up services or carry out any interaction with relevant resources.</p> <p>The video below provides a quick and high-level overview of that the platform's unique selling point.</p> <p>The entry point for accessing the platform's front-end UI is through the login page at the following link:</p> <p>https://aisingapore.run.ai</p> <p>The link above will bring you to the login page:</p> <p></p> <p>To login, click on <code>CONTINUE WITH SSO</code>. You will be redirected to login with your Azure account. After a successful login, you will be brought to the platform's home (<code>Overview</code>) page.</p> <p></p>"},{"location":"guide-for-user/03-mlops-components-platform/#authentication","title":"Authentication","text":"<p>While one can make use of the platform's front-end UI to interact with the Kubernetes cluster in the backend, one might be inclined towards the programmatic approach where a CLI is to be relied on. Run:ai provides a CLI that can be used to interact with the platform's API.</p> <p>To use the CLI, you need to be authenticated. For that, you need the following:</p> <ul> <li>A Kubernetes configuration file a.k.a <code>kubeconfig</code>. This is provided   by the MLOps team.</li> <li>Run:ai CLI to be installed on your local machine (or any client).</li> </ul>"},{"location":"guide-for-user/03-mlops-components-platform/#kubeconfig","title":"<code>kubeconfig</code>","text":"<p>A client that intends to communicate with a Kubernetes cluster would have to rely on a configuration file called <code>kubeconfig</code>. The YAML-formatted <code>kubeconfig</code> would contain information such as cluster endpoints, authentication details, as well as any other access parameters. <code>kubeconfig</code> files are relied on by the <code>kubectl</code> CLI tool for information and credentials to access Kubernetes clusters.</p> <p>In the context of being authenticated with the Run:ai cluster, end-users would be provided with a <code>kubeconfig</code> entailed with the default set of configuration. While you may place this <code>kubeconfig</code> in any (safe) location within your local machine, a reasonable place to place it would be the <code>$HOME/.kube</code> directory.</p> Here is a sample of how the default <code>kubeconfig</code> can look like: <pre><code>apiVersion: v1\nclusters:\n- cluster:\ninsecure-skip-tls-verify: true\nserver: https://runai-cluster.aisingapore.net:6443\nname: runai-cluster-fqdn\ncontexts:\n- context:\ncluster: runai-cluster-fqdn\nuser: runai-authenticated-user\nname: runai-cluster-fqdn\ncurrent-context: runai-cluster-fqdn\nkind: Config\npreferences: {}\nusers:\n- name: runai-authenticated-user\nuser:\nauth-provider:\nconfig:\nairgapped: \"true\"\nauth-flow: remote-browser\nrealm: aisingapore\nclient-id: runai-cli\nidp-issuer-url: https://app.run.ai/auth/realms/aisingapore\nredirect-uri: https://aisingapore.run.ai/oauth-code\nname: oidc\n</code></pre> <p>To understand more on managing configuration for Kubernetes, do refer to the reference document below.</p> Reference Link(s) <ul> <li>Kubernetes Docs - Organizing Cluster Access Using kubeconfig Files</li> </ul>"},{"location":"guide-for-user/03-mlops-components-platform/#runai-cli","title":"Run:ai CLI","text":"<p>With the aforementioned <code>kubeconfig</code> file, we can now use the Run:ai CLI for authentication. We first have to download the CLI.</p> WindowsmacOSLinux <ol> <li>Head over to the Run:ai dashboard.</li> <li>On the top right-hand corner, click on the  <code>Help</code>    icon.</li> <li>Click on <code>Researcher Command Line Interface</code>.</li> <li>Select <code>Windows</code>.</li> <li>Click on  <code>DOWNLOAD</code>, rename the file as <code>runai.exe</code>    and save the file to a location that is included in your <code>PATH</code>    system variable.</li> </ol> <ol> <li>Head over to the Run:ai dashboard.</li> <li>On the top right-hand corner, click on the  <code>Help</code>    icon.</li> <li>Click on <code>Researcher Command Line Interface</code>.</li> <li>Select <code>Mac</code>.</li> <li>Click on  <code>DOWNLOAD</code> and save the file.</li> <li>Run the following commands: <pre><code>$ chmod +x runai\n$ sudo mv runai /usr/local/bin/runai\n</code></pre></li> </ol> <ol> <li>Head over to the Run:ai dashboard.</li> <li>On the top right-hand corner, click on the  <code>Help</code>    icon.</li> <li>Click on <code>Researcher Command Line Interface</code>.</li> <li>Select <code>Linux</code>.</li> <li>Click on  <code>DOWNLOAD</code> and save the file.</li> <li>Run the following commands: <pre><code>$ chmod +x runai\n$ sudo mv runai /usr/local/bin/runai\n</code></pre></li> </ol> <p>To verify your installation, you may run the following command:</p> <pre><code>$ runai version\nVersion: 2.XX.XX\nBuildDate: YYYY-MM-DDThh:mm:ssZ\nGitCommit: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nGoVersion: goX.XX.X\nCompiler: gc\n</code></pre> <p>Now that the CLI has been successfully installed, you can use it to authenticate with the Run:ai cluster.</p> Linux/macOSWindows PowerShell <pre><code>$ export KUBECONFIG=/path/to/provided/kubeconfig\n$ runai login\nGo to the following link in your browser:\n        https://app.run.ai/auth/realms/aisingapore/protocol/openid-connect/auth?access_type=offline&amp;client_id=runai-cli&amp;redirect_uri=https%3A%2F%2Faisingapore.run.ai%2Foauth-code&amp;response_type=code&amp;scope=email+openid+offline_access&amp;state=xxxxxxx\nEnter verification code:\nINFO[0068] Logged in successfully\n</code></pre> <pre><code>$ $KUBECONFIG='/path/to/provided/kubeconfig'\n$ runai login\nGo to the following link in your browser:\n        https://app.run.ai/auth/realms/aisingapore/protocol/openid-connect/auth?access_type=offline&amp;client_id=runai-cli&amp;redirect_uri=https%3A%2F%2Faisingapore.run.ai%2Foauth-code&amp;response_type=code&amp;scope=email+openid+offline_access&amp;state=xxxxxxx\nEnter verification code:\nINFO[0068] Logged in successfully\n</code></pre> <p>As you can see from above, you would be required to use a browser to access the link provided by the CLI. Upon accessing the link, you would be prompted to login with your Azure account. Once you have successfully logged in, you would be provided with a verification code. Copy the verification code and paste it into the terminal.</p> <p>Info</p> <p>What happens in the background when the <code>runai login</code> command is successfully executed is that the <code>kubeconfig</code> file is updated with the necessary authentication details, specifically the <code>id-token</code> and <code>refresh-token</code> fields, which are then used by the <code>kubectl</code> CLI tool to communicate with the Run:ai cluster.</p>"},{"location":"guide-for-user/03-mlops-components-platform/#harbor","title":"Harbor","text":"<p>AI Singapore uses a self-hosted Harbor as the on-premise container image registry.</p> <p>https://registry.aisingapore.net</p> <p></p> <p>To login, use your Azure account username without the domain (if your username is <code>user@aisingapore.org</code>, your username in this context will just be <code>user</code>) and the same password as your Azure account.</p> <p>On a successful login, you should be able to see a list of Harbor projects that you have access to.</p> <p></p>"},{"location":"guide-for-user/03-mlops-components-platform/#docker-cli-authentication","title":"Docker CLI Authentication","text":"<p>While Harbor has its own front-end interface, one may use the Docker CLI to interact with the registry.</p> <pre><code>$ docker login registry.aisingapore.net\nUsername: &lt;YOUR_USERNAME_HERE&gt;\nPassword:\nLogin Succeeded!\n</code></pre> <p>Upon a successful login through the Docker CLI, you can push or pull images to/from the Harbor registry.</p>"},{"location":"guide-for-user/03-mlops-components-platform/#harbor-projects-membership-roles","title":"Harbor Projects, Membership &amp; Roles","text":"<p>For you to push any image to Harbor, you would need authorised access to projects i.e. membership in a project. Projects can be public or private. From the docs:</p> <p>There are two types of project in Harbor:</p> <ul> <li>Public: Any user can pull images from this project. This is a convenient way for you to share repositories with others.</li> <li>Private: Only users who are members of the project can pull images.</li> </ul> <p>Hence, do ensure that you have rightful access to your project team's Harbor project in order for you to push any relevant images that you have built. Do contact the MLOps team (<code>mlops@aisingapore.org</code>) for access matters.</p> <p>Note</p> <p>On your first ever login to Harbor, you would not have any membership access to any projects. This is because projects can only add users who have logged into Harbor at least once. Should you want to be added to a project on Harbor, do notify the MLOps team following your first login.</p> <p>With that said, not all membership is equal i.e. one would need to be assigned the membership roles of either <code>Project Admin</code>, <code>Master</code>, or <code>Developer</code> for pushing permissions.</p> <p>For more information on the aforementioned concepts, do refer to the reference links below.</p> Reference Link(s) <ul> <li>Harbor Docs - Working with Projects</li> <li>Harbor Docs - User Permissions By Role</li> </ul>"},{"location":"guide-for-user/03-mlops-components-platform/#robot-accounts","title":"Robot Accounts","text":"<p>Aside from using your own credentials to interact with the registry, Harbor project admins can create robot accounts to be used for automated workflows. Robot accounts can be provided with customised permissions, configurable according to the needs of the workflows that will be using such accounts.</p> <p>Each project team would be provided with the credentials of a default robot account (contained in a <code>.json</code> file) by the MLOps team.</p>"},{"location":"guide-for-user/03-mlops-components-platform/#elastic-cloud-storage-ecs","title":"Elastic Cloud Storage (ECS)","text":"<p>In the context of AI Singapore's infrastructure, there are two main storage mediums:</p> <ol> <li>AI Singapore's on-premise Network File Storage (NFS)</li> <li>AI Singapore's on-premise object storage, Elastic Cloud Storage (ECS)</li> </ol> <p>The usage of NFS storage is mainly observable through Persistent Volumes (PVs) or virtual machine disks. There is however little to nothing for end-users to configure the usage of NFS storage as most of the setup will be done by AI Singapore's Platforms team.</p> <p>However, to access ECS, there are a number of things that are required of the end-users.</p> Reference Link(s) <ul> <li>Dell Technologies Learning Center - Elastic Cloud Storage</li> <li>IBM Blog - Object vs. File vs. Block Storage: What\u2019s the Difference?</li> </ul>"},{"location":"guide-for-user/03-mlops-components-platform/#aws-cli-for-s3-protocol","title":"AWS CLI for S3 Protocol","text":"<p>AI Singapore's ECS makes use of the S3 protocol and so we can make use of the AWS CLI's S3 commands to interact with the storage system. Instructions for installing the AWS CLI (v2) can be found here.</p> <p>Following installation of the CLI, you would need to configure the settings to be used. The settings can be populated within separate files: <code>config</code> and <code>credentials</code>, usually located under <code>$HOME/.aws</code>. However, we can make do with just populating the <code>credentials</code> file. An example of a <code>credentials</code> file containing credentials for multiple profiles would look like the following:</p> <p>Note</p> <p>The <code>aws_access_key_id</code> and <code>aws_secret_access_key</code> are provided by the DataOps team. The team is reachable at <code>dataops@aisingapore.org</code>.</p> <pre><code>[profile-1]\naws_access_key_id = project-1-user\naws_secret_access_key = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n\n[profile-2]\naws_access_key_id = project-2-user\naws_secret_access_key = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n</code></pre> <p>The <code>profile-1</code> and <code>profile-2</code> are just arbitrary profile names that you can set for your own reference.</p> <p>To list the buckets that a profile as access to, you may run a command similar to the following:</p> <pre><code>$ aws --profile profile-1 --endpoint-url=\"https://necs.nus.edu.sg\" s3 ls\nYYYY-MM-DD hh:mm:ss bucket-1\nYYYY-MM-DD hh:mm:ss bucket-2\n</code></pre> <p>The <code>--endpoint-url</code> flag is required for the AWS CLI to know where to send the requests to. In this case, we are sending requests to AI Singapore's ECS server.</p> <p>Note</p> <p>Some buckets may be hidden when listing buckets. This is due various access permissions that might have been set by administrators. For some buckets, while you may not be able to list them, you may still view the objects that are contained within them.</p> Reference Link(s) <ul> <li>AWS Docs - AWS CLI Configuration and credential file settings</li> <li>AWS CLI Command Reference - s3</li> </ul>"},{"location":"guide-for-user/03-mlops-components-platform/#ecs-robotservice-accounts","title":"ECS Robot/Service Accounts","text":"<p>Project teams may also make use of robot/service accounts to interact with ECS. Robot/service accounts are essentially IAM users that are created by administrators. These accounts are usually created for automated workflows that require access to ECS. Configuring them for the CLI works the same as configuring a regular user account.</p>"},{"location":"guide-for-user/03-mlops-components-platform/#mlflow","title":"MLflow","text":"<p>For model experimentation and tracking needs, AI Singapore mainly relies on MLflow. MLflow is an open-source platform for the machine learning lifecycle. It has several components but we will mainly be using the Tracking server component.</p>"},{"location":"guide-for-user/03-mlops-components-platform/#accessing-tracking-server-dashboard","title":"Accessing Tracking Server Dashboard","text":"<p>Every project has a dedicated MLflow Tracking server, deployed in each project's Kubernetes namespace (or Run:ai project). Also, to access these servers, end-users would need their own credentials, which are provided by the MLOps team. In essence, you would need the following to make use of the MLflow Tracking server:</p> <ul> <li>MLflow Tracking server URL(s)</li> <li>Your own username and password for the same server(s)</li> <li>(Optional) ECS credentials for artifact storage</li> </ul> <p>One would be prompted for a username and password when accessing an MLflow Tracking server for the first time:</p> <p></p> <p>Following a successful login, most end-users would be brought to the <code>Experiments</code> page. Depending on whether one is an admin or a common user, the page would look different. Admin users would be able to view all experiments while common users would only be able to view experiments that they have been provided access to.</p> <p></p> Reference Link(s) <ul> <li>MLflow Docs - MLflow Authentication</li> <li>MLflow Docs - MLflow Tracking (Artifact Stores)</li> </ul>"},{"location":"guide-for-user/03-mlops-components-platform/#logging-to-tracking-server","title":"Logging to Tracking Server","text":"<p>Now, to test out your environment's ability to log to MLflow Tracking server, you can run the sample script that has been provided in this repository. The script can be found at <code>src/mlflow-test.py</code>. The script simply logs a few dummy metrics, parameters, and an artifact to an MLflow Tracking server.</p> Linux/macOSWindows PowerShell <pre><code>$ conda create -n mlflow-test python=3.10.11\n$ conda activate mlflow-test\n$ pip install boto3==1.28.2 mlflow\n$ export MLFLOW_TRACKING_USERNAME=&lt;MLFLOW_TRACKING_USERNAME&gt;\n$ export MLFLOW_TRACKING_PASSWORD=&lt;MLFLOW_TRACKING_PASSWORD&gt;\n$ export MLFLOW_S3_ENDPOINT_URL=\"https://necs.nus.edu.sg\"\n$ export AWS_ACCESS_KEY_ID=&lt;AWS_ACCESS_KEY_ID&gt;\n$ export AWS_SECRET_ACCESS_KEY=&lt;AWS_SECRET_ACCESS_KEY&gt;\n$ python src/mlflow_test.py &lt;MLFLOW_TRACKING_URI&gt; &lt;NAME_OF_DEFAULT_MLFLOW_EXPERIMENT&gt;\n</code></pre> <pre><code>$ conda create -n mlflow-test python=3.10.11\n$ conda activate mlflow-test\n$ pip install boto3==1.28.2 mlflow\n$ $MLFLOW_TRACKING_USERNAME=&lt;MLFLOW_TRACKING_USERNAME&gt;\n$ $MLFLOW_TRACKING_PASSWORD=&lt;MLFLOW_TRACKING_PASSWORD&gt;\n$ $MLFLOW_S3_ENDPOINT_URL=\"https://necs.nus.edu.sg\"\n$ $AWS_ACCESS_KEY_ID=&lt;AWS_ACCESS_KEY_ID&gt;\n$ $AWS_SECRET_ACCESS_KEY=&lt;AWS_SECRET_ACCESS_KEY&gt;\n$ python src/mlflow_test.py &lt;MLFLOW_TRACKING_URI&gt; &lt;NAME_OF_DEFAULT_MLFLOW_EXPERIMENT&gt;\n</code></pre> <p>A successful run of the script would present you with an experiment run that looks similar to the following:</p> <p></p> Reference Link(s) <ul> <li>MLflow Docs - MLflow Tracking</li> </ul>"},{"location":"guide-for-user/03-mlops-components-platform/#local-virtual-environments","title":"Local Virtual Environments","text":"<p>While we will be making use of AI Singapore's remote infrastructure to carry out some workflows, we can still make use of our local machine to execute some of the steps of the end-to-end machine learning workflow. Hence, we can begin by creating a virtual environment that will contain all the dependencies required for this guide.</p> <pre><code>$ conda env create -f {{cookiecutter.repo_name}}-conda-env.yaml\n</code></pre>"},{"location":"guide-for-user/04-dev-wksp/","title":"Development Workspace","text":"<p>An advantage presented by orchestration platforms is that you can utilise the Kubernetes cluster's resources for your development and engineering works instead of your own resources.</p> <p>We can make use of Run:ai workspaces to spin up VSCode or JupyterLab servers with which cluster resources can be dedicated.</p> <p>While there exist the option for engineers to set up either a VSCode or JupyterLab service (or both), the former would suffice. Reason being VSCode is an excellent code editor with integrated terminal capabilities and it can also work with Jupyter notebooks. JupyterLab on the other hand, while being the best interface for Jupyter notebooks, has subpar UX for its terminal and code editor. That is to be expected however as it is dedicated to the Jupyter ecosystem.</p>"},{"location":"guide-for-user/04-dev-wksp/#workspace-building-blocks","title":"Workspace Building Blocks","text":"<p>Run:ai workspaces relies on an abstration layer of building blocks. These building blocks allows for reusable setups where separate configuration can be mixed and match to provide end-users with their ideal setup.</p> <p>Every workspace intance are built using the following building blocks:</p> <ul> <li>environment</li> <li>data source</li> <li>compute resource</li> </ul> <p>On top of using and reusing building blocks that have existed prior, you may create your own which in turn can be used by other users.</p> <p>Do refer to Run:ai's documentation on more information for each of these building blocks.</p>"},{"location":"guide-for-user/04-dev-wksp/#vscode","title":"VSCode","text":""},{"location":"guide-for-user/04-dev-wksp/#prebuilt-vscode-server","title":"Prebuilt VSCode Server","text":"<p>Every end-user of Run:ai would be able to quickly spin up a VSCode server workspace using prebuilt blocks. While the steps for creating a workspace are detailed on Run:ai's documentation, listed below are the recommended environment, compute resource, and data source that you may make use of to spin up your first VSCode workspace, in the context of AI Singapore's infrastructure.</p> <ul> <li> <p>Workspace name: <code>&lt;YOUR_HYPHENATED_NAME&gt;-vscode</code></p> </li> <li> <p>Environment: <code>aisg-vscode-server-v4-16-1</code></p> </li> <li> <p>Compute Resource: <code>cpu-mid</code></p> </li> <li> <p>Data Source: The persistent volume claim (PVC) that is dedicated   to your project. For a project named <code>sample-project</code>, you may make use of   <code>sample-project-pvc</code>.</p> </li> </ul> <p>Once you have selected the blocks, you can proceed to create the workspace and you will be redirected to the workspaces page. On this page, you may view the status of the workspace that you have just created.</p> <p></p> <p>Once the workspace is active (indicated by a green status), you may access the workspace by clicking on the <code>CONNECT</code> button and choosing <code>VSCode</code>. This will open up a new browser tab for the VSCode server, accessible through a URL that follows the following convention: <code>&lt;NAME_OF_PROJECT&gt;-&lt;NAME_OF_WORKSPACE&gt;.runai.aisingapore.net</code>. However, you cannot access the VSCode interface just yet; you will prompted for a password.</p> <p></p> <p>To retrieve the password, head over to your terminal and run the following command:</p> Linux/macOSWindows PowerShell <pre><code>$ runai exec &lt;YOUR_HYPHENATED_NAME&gt;-vscode -p {{cookiecutter.runai_proj_name}} -- cat /home/coder/.config/code-server/config.yaml\nbind-addr: 127.0.0.1:8080\nauth: password\npassword: xxxxxxxxxxxxxxxxxxxxxxxx\ncert: false\n</code></pre> <pre><code>$ runai exec &lt;YOUR_HYPHENATED_NAME&gt;-vscode -p {{cookiecutter.runai_proj_name}} -- cat /home/coder/.config/code-server/config.yaml\nbind-addr: 127.0.0.1:8080\nauth: password\npassword: xxxxxxxxxxxxxxxxxxxxxxxx\ncert: false\n</code></pre> <p>Copy the value for the <code>password</code> field into the prompt on the browser and you should be directed to the VSCode server welcome tab.</p> <p></p> <p>However, the default folder that your VSCode server is accessing is the home directory of the container's user, which is <code>/home/coder</code>. Since a container's filesystem is ephemeral, we have to make use of a persistent storage. Otherwise, any changes written within the container's filesystem will be lost once the container is terminated. For persistence, we look into using a persistent volume claim, which if you have followed the recommendations above, a PVC would have been mounted to the container.</p> <p>Reference(s):</p> <ul> <li>Kubernetes Docs - Persistent Volumes</li> </ul>"},{"location":"guide-for-user/04-dev-wksp/#persistent-workspaces","title":"Persistent Workspaces","text":"<p>As mentioned, a PVC should be attached to the workspaces to persist changes to the filesystems. If a PVC is attached, the usual path to access it would be <code>/&lt;NAME_OF_DATA_SOURCE&gt;</code>. For example, if the name of the data source is called <code>sample-project-pvc</code>, the path to the PVC that has been mounted to the container would be <code>/sample-project-pvc</code>.</p> VSCode Server Terminal <pre><code>$ ls -la / | grep \"pvc\"\n</code></pre> <p>By default, the PVCs would contain a <code>workspaces</code> directory with which you can create a subdirectory for yourself treat it as your own personal workspace, where all your work and other relevant assets can be persisted.</p> VSCode Server Terminal <pre><code>$ cd /{{cookiecutter.runai_proj_name}}_pvc/workspaces\n$ mkdir &lt;YOUR_HYPHENATED_NAME&gt;\n</code></pre>"},{"location":"guide-for-user/04-dev-wksp/#git-from-vscode","title":"Git from VSCode","text":"<p>To clone or push to Git repositories within the VSCode integrated terminal, it is recommended that you first disable VSCode's Git authentication handler:</p> <p>Git by default is installed in the VSCode server image. One thing to take note is that as the persistent storage would be accessible by the rest of your project team members, you should only use the <code>HTTPS</code> protocol to clone the repository as opposed to creating and using an <code>SSH</code> key within the VSCode server.</p> <p>Now, let's clone your repository from the remote:</p> VSCode Server Terminal <pre><code>$ cd /{{cookiecutter.runai_proj_name}}/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;\n$ git clone &lt;REMOTE_URL_HTTPS&gt;\n$ cd {{cookiecutter.repo_name}}\n</code></pre>"},{"location":"guide-for-user/04-dev-wksp/#extensions-for-vscode","title":"Extensions for VSCode","text":"<p>You can install a multitude of extensions for your VSCode service but there are a couple that would be crucial for your workflow, especially if you intend to use Jupyter notebooks within the VSCode environment.</p> <ul> <li><code>ms-python.python</code>:   Official extension by Microsoft for rich support for many things   Python.</li> <li><code>ms-toolsai.jupyter</code>:   Official extension by Microsoft for Jupyter support.</li> </ul> <p>Attention</p> <p>Do head over here on how to enable the usage of virtual <code>conda</code> environments within VSCode.</p>"},{"location":"guide-for-user/04-dev-wksp/#customising-vscode-server","title":"Customising VSCode Server","text":"<p>The prebuilt VSCode image was built using a Dockerfile which can be found in this repository at <code>docker/vscode-server/vscode-server.Dockerfile</code>. The Dockerfile was customised by the MLOps team at AI Singapore to include tools deemed relevant for machine learning development, in the context of the organisation's tech stack and infrastructure.</p> <p>Aside from the server itself, here are some of the tools that are included in the image:</p> <ul> <li>Git</li> <li>Miniconda</li> <li><code>kubectl</code></li> <li>Helm</li> <li>AWS CLI</li> </ul> <p>Often times, project teams would like to further customise the VSCode server image to their liking. One can edit the Dockerfile and build the custom image:</p> Linux/macOSWindows PowerShell <pre><code>$ docker build \\\n-t {{cookiecutter.harbor_registry_project_path}}/vscode-server-custom:0.1.0 \\\n-f docker/vscode-server/vscode-server.Dockerfile \\\n--platform linux/amd64 .\n$ docker push {{cookiecutter.harbor_registry_project_path}}/vscode-server-custom:0.1.0\n</code></pre> <pre><code>$ docker build `\n    -t {{cookiecutter.harbor_registry_project_path}}/vscode-server-custom:0.1.0 `\n    -f docker/vscode-server/vscode-server.Dockerfile `\n    --platform linux/amd64 .\n$ docker push {{cookiecutter.harbor_registry_project_path}}/vscode-server-custom:0.1.0\n</code></pre>"},{"location":"guide-for-user/04-dev-wksp/#jupyterlab","title":"JupyterLab","text":"<p>Attention</p> <p>Setting up of a JupyterLab server is optional and not needed as a VSCode server is sufficient as a developer workspace. Resources are limited so use only what you need.</p>"},{"location":"guide-for-user/04-dev-wksp/#prebuilt-jupyterlab-server","title":"Prebuilt JupyterLab Server","text":"<p>While Jupyter Notebooks are viewable, editable and executable within a VSCode environment, most are still more familiar with Jupyter's interface for interacting with or editing notebooks. We can spin up a JupyterLab using the following recommended blocks:</p> <ul> <li> <p>Workspace name: <code>&lt;YOUR_HYPHENATED_NAME&gt;-jupyterlab</code></p> </li> <li> <p>Environment: <code>aisg-jupyterlab-server-0-1-0</code></p> </li> <li> <p>Compute Resource: <code>cpu-mid</code></p> </li> <li> <p>Data Source: The PVC that is dedicated to your project. For a   sample project, you may make use of <code>sample-project-pvc</code>.</p> </li> </ul> <p>Attention</p> <p>Under the <code>Environment</code> block, there is an expandable section called <code>More settings</code>. Under this section, you can provide more arguments for a container that will be spun up for the workspace. For the JupyterLab interface to be able to access any PVC mounted to the container, you should include the following argument: <code>--NotebookApp.notebook_dir=\"/path/to/pvc\"</code>.</p> <p>Once you have selected the blocks, you can proceed to create the workspace and you will be redirected to the workspaces page. On this page, you may view the status of the workspace that you have just created.</p> <p></p> <p>Once the workspace is active (indicated by a green status), you may access the workspace by clicking on the <code>CONNECT</code> button and choosing <code>Jupyter</code>. This will open up a new browser tab for the VSCode server, accessible through a URL that follows the following convention: <code>&lt;NAME_OF_PROJECT&gt;-&lt;NAME_OF_WORKSPACE&gt;.runai.aisingapore.net</code>. However, you cannot access the JupyterLab interface just yet; you will prompted for a token.</p> <p></p> <p>To retrieve the token, head over to your terminal and run the following command:</p> Linux/macOSWindows PowerShell <pre><code>$ runai logs &lt;YOUR_HYPHENATED_NAME&gt;-jupyterlab -p  {{cookiecutter.runai_proj_name}} | grep \"lab?token\"\n[I YYYY-MM-DD hh:mm:ss ServerApp] http://&lt;NAME_OF_WORKSPACE&gt;-X-X:8888/lab?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n[I YYYY-MM-DD hh:mm:ss ServerApp]     http://127.0.0.1:8888/lab?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n        http://&lt;NAME_OF_WORKSPACE&gt;-X-X:8888/lab?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n        http://127.0.0.1:8888/lab?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n</code></pre> <pre><code>$ runai logs &lt;YOUR_HYPHENATED_NAME&gt;-jupyterlab -p  {{cookiecutter.runai_proj_name}} | Where-Object{$_ -match \"lab?token\"}\n[I YYYY-MM-DD hh:mm:ss ServerApp] http://&lt;NAME_OF_WORKSPACE&gt;-X-X:8888/lab?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n[I YYYY-MM-DD hh:mm:ss ServerApp]     http://127.0.0.1:8888/lab?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n        http://&lt;NAME_OF_WORKSPACE&gt;-X-X:8888/lab?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n        http://127.0.0.1:8888/lab?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n</code></pre> <p>Copy the token value into the prompt on the browser and you should be directed to a view like such:</p> <p></p> <p>Reference(s):</p> <ul> <li>Jupyter Server Docs - Config file and command line options</li> </ul> <p>Info</p> <p>Do head over here on how to enable the usage of virtual <code>conda</code> environments within JupyterLab.</p>"},{"location":"guide-for-user/04-dev-wksp/#customising-jupyterlab-server","title":"Customising JupyterLab Server","text":"<p>Of course, like with the VSCode server, one can work on a customised JupyterLab server image. The Dockerfile for the prebuilt JupyterLab server as well as any associated files can be found under <code>docker/jupyterlab-server</code>:</p> Linux/macOSWindows PowerShell <pre><code>$ docker build \\\n-t {{cookiecutter.harbor_registry_project_path}}/jupyterlab-server-custom:0.1.0 \\\n-f docker/jupyterlab-server/jupyterlab-server.Dockerfile \\\n--platform linux/amd64 .\n$ docker push {{cookiecutter.harbor_registry_project_path}}/jupyterlab-server-custom:0.1.0\n</code></pre> <pre><code>$ docker build `\n    -t {{cookiecutter.harbor_registry_project_path}}/jupyterlab-server-custom:0.1.0 `\n    -f docker/jupyterlab-server/jupyterlab-server.Dockerfile `\n    --platform linux/amd64 .\n$ docker push {{cookiecutter.harbor_registry_project_path}}/jupyterlab-server-custom:0.1.0\n</code></pre>"},{"location":"guide-for-user/04-dev-wksp/#using-docker-within-kubernetes","title":"Using Docker within Kubernetes","text":"<p>Caution</p> <p>Since these development environments are essentially pods deployed within a Kubernetes cluster, using Docker within the pods themselves is not feasible by default and while possible, should be avoided.</p> <p>Reference(s):</p> <ul> <li>Using Docker-in-Docker for your CI or testing environment? Think twice. - jpetazzo</li> </ul>"},{"location":"guide-for-user/05-virtual-env/","title":"Virtual Environment","text":"<p>While the Docker images you will be using to run experiments on Run:ai would contain the <code>conda</code> environments you would need, you can also create these virtual environments within your development environment, and have it be persisted. The following set of commands allows you to create the <code>conda</code> environment and store the packages within your own workspace directory:</p> <ul> <li> <p>First, have VSCode open the repository that you have cloned   previously by heading over to the top left hand corner, selecting   <code>File &gt; Open Folder...</code>, and entering the path to the repository.   In this case, you should be navigating to the folder   <code>/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/{{cookiecutter.repo_name}}</code>.</p> </li> <li> <p>Now, let's initialise <code>conda</code> for the bash shell, and create   the virtual environment specified in   <code>{{cookiecutter.repo_name}}-conda-env.yaml</code>.</p> </li> </ul> VSCode Server Terminal <pre><code>(base) $ conda env create -f {{cookiecutter.repo_name}}-conda-env.yaml \\\n-p /&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/conda_envs/{{cookiecutter.repo_name}}-conda-env\n</code></pre> <ul> <li>After creating the <code>conda</code> environment, let's create a permanent   alias for easy activation.</li> </ul> VSCode Server Terminal <pre><code>(base) $ echo 'alias {{cookiecutter.repo_name}}-conda-env=\"conda activate /&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/conda_envs/{{cookiecutter.repo_name}}-conda-env\"' &gt;&gt; ~/.bashrc\n(base) $ source ~/.bashrc\n(base) $ {{cookiecutter.repo_name}}-conda-env\n({{cookiecutter.repo_name}}-conda-env) $ # conda environment has been activated\n</code></pre> <p>Tip</p> <p>If you encounter issues in trying to install Python libraries, do ensure that the amount of resources allocated to the VSCode server is sufficient. Installation of libraries from PyPI tends to fail when there's insufficient memory. For starters, dedicate 4GB of memory to the service:</p> <p>Another way is to add the flag <code>--no-cache-dir</code> for your <code>pip install</code> executions. However, there's no similar flag for <code>conda</code> at the moment so the above is a blanket solution.</p> <p>Reference(s):</p> <ul> <li><code>conda</code> Docs - Managing environments</li> <li>StackOverflow - \"Pip install killed - out of memory - how to get around it?\"</li> <li>phoenixNAP - Linux alias Command: How to Use It With Examples</li> </ul>"},{"location":"guide-for-user/05-virtual-env/#jupyter-kernel-for-vscode","title":"Jupyter Kernel for VSCode","text":"<p>While it is possible for VSCode to make use of different virtual Python environments, some other additional steps are required for the VSCode server to detect the <code>conda</code> environments that you would have created.</p> <ul> <li> <p>Ensure that you are in a project folder which you intend to work   on. You can open a folder through <code>File &gt; Open Folder...</code>.   In this case, you should be navigating to the folder   <code>/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/{{cookiecutter.repo_name}}</code>.</p> </li> <li> <p>Install the VSCode extensions   <code>ms-python.python</code>   and   <code>ms-toolsai.jupyter</code>.   After installation of these extensions, restart VSCode by using   the shortcut <code>Ctrl + Shift + P</code>, entering <code>Developer: Reload Window</code> in the   prompt and pressing <code>Enter</code> following that.</p> </li> <li> <p>Ensure that you have   <code>ipykernel</code>   installed in the <code>conda</code> environment that you intend to use.   This template by default lists the library as a dependency under   <code>{{cookiecutter.repo_name}}-conda-env.yaml</code>. You can check for the   library like so:</p> </li> </ul> VSCode Server Terminal <pre><code>$ conda activate /&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/conda_envs/{{cookiecutter.repo_name}}-conda-env\n$ conda list | grep \"ipykernel\"\nipykernel  X.X.X  pypi_0  pypi\n</code></pre> <ul> <li> <p>Now enter <code>Ctrl + Shift + P</code> again and execute <code>Python: Select Interpreter</code>.   Provide the path to the Python executable within the <code>conda</code>   environment that you intend to use, something like so:   <code>path/to/conda_env/bin/python</code>.</p> </li> <li> <p>Open up any Jupyter notebook and click on the button that says   <code>Select Kernel</code> on the top right hand corner. You will be presented   with a selection of Python interpreters. Select the one that   corresponds to the environment you intend to use.</p> </li> <li> <p>Test out the kernel by running the cells in the sample notebook   provided under <code>notebooks/sample-pytorch-notebook.ipynb</code>.</p> </li> </ul>"},{"location":"guide-for-user/05-virtual-env/#jupyter-kernel-for-jupyterlab","title":"Jupyter Kernel for JupyterLab","text":"<p>The same with the VSCode server, the JupyterLab server would not by default detect <code>conda</code> environments. You would have to specify to the JupyterLab installation the <code>ipython</code> kernel existing within your <code>conda</code> environment.</p> <ul> <li> <p>Open up a   terminal within JupyterLab.</p> </li> <li> <p>Activate the <code>conda</code> environment in question and ensure that you have   <code>ipykernel</code>   installed in the <code>conda</code> environment that you intend to use.   This template by default lists the library as a dependency under   <code>{{cookiecutter.repo_name}}-conda-env.yaml</code>. You can check for the   library like so:</p> </li> </ul> JupyterLab Terminal <pre><code>$ conda activate /&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/conda_envs/{{cookiecutter.repo_name}}-conda-env\n$ conda list | grep \"ipykernel\"\nipykernel  6.9.2  pypi_0  pypi\n</code></pre> <ul> <li>Within the <code>conda</code> environment, execute the following:</li> </ul> JupyterLab Terminal <pre><code>$ ipython kernel install --name \"{{cookiecutter.repo_name}}-conda-env\" --user\n</code></pre> <ul> <li> <p>Refresh the page.</p> </li> <li> <p>Open up the sample notebook provided under <code>notebooks/sample-pytorch-notebook.ipynb</code>.</p> </li> <li> <p>Within each Jupyter notebook, you can select the kernel of   specific <code>conda</code> environments that you intend to use by heading to   the toolbar under   <code>Kernel</code> -&gt; <code>Change Kernel...</code>.</p> </li> </ul> <p></p> <ul> <li>Test out the kernel by running the cells in the sample notebook.</li> </ul> <p>Reference(s):</p> <ul> <li>Jupyter Docs - Kernels (Programming Languages)</li> </ul>"},{"location":"guide-for-user/06-data-storage-versioning/","title":"Data Storage &amp; Versioning","text":""},{"location":"guide-for-user/06-data-storage-versioning/#sample-data","title":"Sample Data","text":"<p>While you may have your own project data to work with, for the purpose of following through with this template guide, let's download the sample data for the sample problem statement at hand within our VSCode server workspace.</p> VSCode Server Terminal <pre><code>$ mkdir -p /&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/data &amp;&amp; cd \"$_\"\n$ wget https://storage.googleapis.com/aisg-mlops-pub-data/kapitan-hull/mnist-pngs-data-aisg.zip\n$ unzip mnist-pngs-data-aisg.zip\n</code></pre> <p>Info</p> <p>The sample data for this guide's problem statement is made accessible to the public. Hence any team or individual can download it. It is highly likely that your project's data is not publicly accessible and neither should it be, especially if it is a 100E project.</p> <p>In the next section, we will work towards processing this set of raw data and eventually training an image classification model.</p>"},{"location":"guide-for-user/07-job-orchestration/","title":"Job Orchestration","text":"<p>Even though we can set up development workspaces to execute jobs and workflows, these environments often have limited access to resources. To carry out heavier workloads, we encourage the usage of job orchestration features that Run:ai offers.</p> <p>Jobs are submitted to the Kubernetes cluster through Run:ai and executed within Docker containers. Using the images specified upon job submission, Kubernetes pods are spun up to execute the entry points or commands defined, tapping on to the cluster's available resources.</p> <p>Any jobs that are submitted to Run:ai can be tracked and monitored through Run:ai's dashboard.</p>"},{"location":"guide-for-user/07-job-orchestration/#pipeline-configuration","title":"Pipeline Configuration","text":"<p>In this template, Hydra is the configuration framework of choice for the data preparation and model training pipelines (or any pipelines that doesn't belong to the model serving aspects).</p> <p>The configurations for logging, pipelines and hyperparameter tuning can be found under <code>conf/base</code>. These YAML files are then referred to by Hydra or general utility functions (<code>src/{{cookiecutter.src_package_name}}/general_utils.py</code>) for loading of parameters and configurations. The defined default values can be overridden through the CLI.</p> <p>Attention</p> <p>It is recommended that you have a basic understanding of Hydra's concepts before you move on.</p> <p>Reference(s):</p> <ul> <li>Hydra Docs - Basic Override Syntax</li> </ul>"},{"location":"guide-for-user/07-job-orchestration/#data-preparation-preprocessing","title":"Data Preparation &amp; Preprocessing","text":"<p>To process the sample raw data, we will be spinning up a job on Run:ai, using the CLI. This job will be using a Docker image that will be built from a Dockerfile (<code>docker/{{cookiecutter.src_package_name}}-data-prep.Dockerfile</code>) provided in this template:</p> Linux/macOSWindows PowerShell <pre><code>$ docker build \\\n-t {{cookiecutter.harbor_registry_project_path}}/data-prep:0.1.0 \\\n-f docker/{{cookiecutter.repo_name}}-data-prep.Dockerfile \\\n--platform linux/amd64 .\n$ docker push {{cookiecutter.harbor_registry_project_path}}/data-prep:0.1.0\n</code></pre> <pre><code>$ docker build `\n    -t {{cookiecutter.harbor_registry_project_path}}/data-prep:0.1.0 `\n    -f docker/{{cookiecutter.repo_name}}-data-prep.Dockerfile `\n    --platform linux/amd64 .\n$ docker push {{cookiecutter.harbor_registry_project_path}}/data-prep:0.1.0\n</code></pre> <p>Now that we have the Docker image pushed to the registry, we can submit a job using that image to Run:ai:</p> Linux/macOSWindows PowerShell <pre><code>$ runai submit \\\n--job-name-prefix &lt;YOUR_HYPHENATED_NAME&gt;-data-prep \\\n-i {{cookiecutter.harbor_registry_project_path}}/data-prep:0.1.0 \\\n--working-dir /&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/{{cookiecutter.repo_name}} \\\n--pvc &lt;NAME_OF_DATA_SOURCE&gt;:/&lt;NAME_OF_DATA_SOURCE&gt; \\\n--cpu 2 \\\n--memory 4G \\\n--command -- '/bin/bash -c \"source activate {{cookiecutter.repo_name}} &amp;&amp; python src/process_data.py process_data.raw_data_dir_path=/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/data/mnist-pngs-data-aisg process_data.processed_data_dir_path=/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/data/processed/mnist-pngs-data-aisg-processed\"'\n</code></pre> <pre><code>$ runai submit `\n    --job-name-prefix &lt;YOUR_HYPHENATED_NAME&gt;-data-prep `\n    -i {{cookiecutter.harbor_registry_project_path}}/data-prep:0.1.0 `\n    --working-dir /&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/{{cookiecutter.repo_name}} `\n    --pvc &lt;NAME_OF_DATA_SOURCE&gt;:/&lt;NAME_OF_DATA_SOURCE&gt; `\n    --cpu 2 `\n    --memory 4G `\n    --command -- \"/bin/bash -c 'source activate {{cookiecutter.repo_name}} &amp;&amp; python src/process_data.py process_data.raw_data_dir_path=/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/data/mnist-pngs-data-aisg process_data.processed_data_dir_path=/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/data/processed/mnist-pngs-data-aisg-processed'\"\n</code></pre> <p>After some time, the data processing job should conclude and we can proceed with training the predictive model. The processed data is exported to the directory <code>/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/data/processed/mnist-pngs-data-aisg-processed</code>. We will be passing this path to the model training workflows.</p>"},{"location":"guide-for-user/07-job-orchestration/#model-training","title":"Model Training","text":"<p>Now that we have processed the raw data, we can look into training the sentiment classification model. The script relevant for this section is <code>src/train_model.py</code>. In this script, you can see it using some utility functions from <code>src/{{cookiecutter.src_package_name}}/general_utils.py</code> as well, most notably the functions for utilising MLflow utilities for tracking experiments. Let's set up the tooling for experiment tracking before we start model experimentation.</p>"},{"location":"guide-for-user/07-job-orchestration/#experiment-tracking","title":"Experiment Tracking","text":"<p>In the module <code>src/{{cookiecutter.src_package_name}}/general_utils.py</code>, the functions <code>mlflow_init</code> and <code>mlflow_log</code> are used to initialise MLflow experiments as well as log information and artifacts relevant for a run to a remote MLflow Tracking server. An MLflow Tracking server is usually set up within the Run:ai project's namespace for projects that requires model experimentation. Artifacts logged through the MLflow API can be uploaded to ECS buckets, assuming the client is authorised for access to ECS.</p> <p>Note</p> <p>The username and password for the MLflow Tracking server can be retrieved from the MLOps team or your team lead.</p> <p>To log and upload artifacts to ECS buckets through MLflow, you need to ensure that the client has access to the credentials of an account that can write to a bucket.</p> <p>Reference(s):</p> <ul> <li>MLflow Docs - Tracking</li> <li>MLflow Docs - Tracking (Artifact Stores)</li> </ul>"},{"location":"guide-for-user/07-job-orchestration/#container-for-experiment-job","title":"Container for Experiment Job","text":"<p>Before we submit a job to Run:ai to train our model, we need to build the Docker image to be used for it:</p> Linux/macOSWindows PowerShell <pre><code>$ docker build \\\n-t {{cookiecutter.harbor_registry_project_path}}/model-training:0.1.0 \\\n-f docker/{{cookiecutter.repo_name}}-model-training.Dockerfile \\\n--platform linux/amd64 .\n$ docker push {{cookiecutter.harbor_registry_project_path}}/model-training:0.1.0\n</code></pre> <pre><code>$ docker build `\n    -t {{cookiecutter.harbor_registry_project_path}}/model-training:0.1.0 `\n    -f docker/{{cookiecutter.repo_name}}-model-training.Dockerfile `\n    --platform linux/amd64 .\n$ docker push {{cookiecutter.harbor_registry_project_path}}/model-training:0.1.0\n</code></pre> <p>Now that we have the Docker image pushed to the registry, we can run a job using it:</p> Linux/macOSWindows PowerShell <pre><code>$ runai submit \\\n--job-name-prefix &lt;YOUR_HYPHENATED_NAME&gt;-train \\\n-i {{cookiecutter.harbor_registry_project_path}}/model-training:0.1.0 \\\n--working-dir /home/aisg/{{cookiecutter.repo_name}} \\\n--pvc &lt;NAME_OF_DATA_SOURCE&gt;:/&lt;NAME_OF_DATA_SOURCE&gt; \\\n--cpu 2 \\\n--memory 4G \\\n-e AWS_ACCESS_KEY_ID=SECRET:s3-credentials,accessKeyId \\\n-e AWS_SECRET_ACCESS_KEY=SECRET:s3-credentials,secretAccessKey \\\n-e MLFLOW_S3_ENDPOINT_URL=\"https://necs.nus.edu.sg\" \\\n-e MLFLOW_TRACKING_USERNAME=&lt;YOUR_MLFLOW_USERNAME&gt; \\\n-e MLFLOW_TRACKING_PASSWORD=&lt;YOUR_MLFLOW_PASSWORD&gt; \\\n--command -- '/bin/bash -c \"source activate {{cookiecutter.repo_name}} &amp;&amp; python src/train_model.py train_model.data_dir_path=/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/data/processed/mnist-pngs-data-aisg-processed train_model.setup_mlflow=true train_model.mlflow_tracking_uri=&lt;MLFLOW_TRACKING_URI&gt; train_model.mlflow_exp_name=&lt;NAME_OF_DEFAULT_MLFLOW_EXPERIMENT&gt; train_model.model_checkpoint_dir_path=/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/{{cookiecutter.repo_name}}/models train_model.epochs=3\"'\n</code></pre> <pre><code>$ runai submit `\n    --job-name-prefix &lt;YOUR_HYPHENATED_NAME&gt;-train `\n    -i {{cookiecutter.harbor_registry_project_path}}/model-training:0.1.0 `\n    --working-dir /home/aisg/{{cookiecutter.repo_name}} `\n    --pvc &lt;NAME_OF_DATA_SOURCE&gt;:/&lt;NAME_OF_DATA_SOURCE&gt; `\n    --cpu 2 `\n    --memory 4G `\n    -e AWS_ACCESS_KEY_ID=SECRET:s3-credentials,accessKeyId `\n    -e AWS_SECRET_ACCESS_KEY=SECRET:s3-credentials,secretAccessKey `\n    -e MLFLOW_S3_ENDPOINT_URL=\"https://necs.nus.edu.sg\" `\n    -e MLFLOW_TRACKING_USERNAME=&lt;YOUR_MLFLOW_USERNAME&gt; `\n    -e MLFLOW_TRACKING_PASSWORD=&lt;YOUR_MLFLOW_PASSWORD&gt; `\n    --command -- \"/bin/bash -c 'source activate {{cookiecutter.repo_name}} &amp;&amp; python src/train_model.py train_model.data_dir_path=/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/data/processed/mnist-pngs-data-aisg-processed train_model.setup_mlflow=true train_model.mlflow_tracking_uri=&lt;MLFLOW_TRACKING_URI&gt; train_model.mlflow_exp_name=&lt;NAME_OF_DEFAULT_MLFLOW_EXPERIMENT&gt; train_model.model_checkpoint_dir_path=/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/{{cookiecutter.repo_name}}/models train_model.epochs=3'\"\n</code></pre> <p>Once you have successfully run an experiment, you may inspect the run on the MLflow Tracking server. Through the MLflow Tracking server interface, you can view the metrics and parameters logged for the run, as well as download the artifacts that have been uploaded to the ECS bucket. You can also compare runs with each other.</p> <p></p> <p>Tip</p> <p>Every job submitted with <code>runai submit</code> is assigned a unique ID, and a unique job name if the <code>--job-name-prefix</code> is used. The <code>mlflow_init</code> function within the <code>general_utils.py</code> module tags every experiment name with the job's name and UUID as provided by Run:ai, with the tags <code>job_uuid</code> and <code>job_name</code>. This allows you to easily identify the MLflow experiment runs that are associated with each Run:ai job. You can filter for MLflow experiment runs associated with a specific Run:ai job by using MLflow's search filter expressions and API.</p> <p>Reference(s):</p> <ul> <li>Run:ai Docs - Environment Variables inside a Run:ai Workload</li> <li>MLflow Docs - Search Runs</li> </ul> <p>Info</p> <p>If your project has GPU quotas assigned to it, you can make use of it by specifying the <code>--gpu</code> flag in the <code>runai submit</code> command. As part of Run:ai's unique selling point, you can also specify fractional values, which would allow you to utilise a fraction of a GPU. This is useful for projects that require a GPU for training, but do not require the full capacity of a GPU.</p>"},{"location":"guide-for-user/07-job-orchestration/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>For many ML problems, we would be bothered with finding the optimal parameters to train our models with. While we are able to override the parameters for our model training workflows, imagine having to sweep through a distribution of values. For example, if you were to seek for the optimal learning rate within a log space, we would have to execute <code>runai submit</code> a myriad of times manually, just to provide the training script with a different learning rate value each time. It is reasonable that one seeks for ways to automate this workflow.</p> <p>Optuna is an optimisation framework designed for ML use-cases. Its features includes:</p> <ul> <li>ease of modularity,</li> <li>optimisation algorithms for searching the best set of parameters,</li> <li>and paralellisation   capabilities for faster sweeps.</li> </ul> <p>In addition, Hydra has a plugin for utilising Optuna which further translates to ease of configuration. To use Hydra's plugin for Optuna, we have to provide further overrides within the YAML config, and this is observed in <code>conf/base/pipelines.yaml</code>:</p> <pre><code>defaults:\n- override hydra/sweeper: optuna\n- override hydra/sweeper/sampler: tpe\n\nhydra:\nsweeper:\nsampler:\nseed: 55\ndirection: [\"minimize\", \"maximize\"]\nstudy_name: \"image-classification\"\nstorage: null\nn_trials: 3\nn_jobs: 1\nparams:\ntrain_model.lr: range(0.9,1.7,step=0.1)\ntrain_model.gamma: choice(0.7,0.8,0.9)\n</code></pre> <p>These fields are used by the Optuna Sweeper plugin to configure the Optuna study.</p> <p>Attention</p> <p>The fields defined are terminologies used by Optuna. Therefore, it is recommended that you understand the basics of the tool. This overview video covers well on the concepts brought upon by Optuna.</p> <p>Here are the definitions for some of the fields:</p> <ul> <li><code>params</code> is used to specify the parameters to be tuned, and the values   to be searched through</li> <li><code>n_trials</code> specifies the number of trials to be executed</li> <li><code>n_jobs</code> specifies the number of trials to be executed in parallel</li> </ul> <p>As to how the training script would work towards training a model with the best set of parameters, there are two important lines from two different files that we have to pay attention to.</p> <p><code>src/train_model.py</code></p> <pre><code>...\n    return curr_test_loss, curr_test_accuracy\n...\n</code></pre> <p><code>conf/base/pipelines.yaml</code></p> <pre><code>...\ndirection: [\"minimize\", \"maximize\"]\n...\n</code></pre> <p>In the training script the returned variables are to contain values that we seek to optimise for. In this case, we seek to minimise the loss and maximise the accuracy. The <code>hydra.sweeper.direction</code> field in the YAML config is used to specify the direction that those variables are to optimise towards, defined in a positional manner within a list.</p> <p>An additional thing to take note of is that for each trial where a different set of parameters are concerned, a new MLflow run has to be initialised. However, we need to somehow link all these different runs together so that we can compare all the runs within a single Optuna study (set of trials). How we do this is that we provide each trial with the same tag to be logged to MLflow (<code>hptuning_tag</code>) which would essentially be the date epoch value of the moment you submitted the job to Run:ai. This tag is defined using the environment value <code>MLFLOW_HPTUNING_TAG</code>. This tag is especially useful if you are executing the model training job out of the Run:ai platform, as the <code>JOB_NAME</code> and <code>JOB_UUID</code> environment variables would not be available by default.</p> Linux/macOSWindows PowerShell <pre><code>$ runai submit \\\n--job-name-prefix &lt;YOUR_HYPHENATED_NAME&gt;-train \\\n-i {{cookiecutter.harbor_registry_project_path}}/model-training:0.1.0 \\\n--working-dir /home/aisg/{{cookiecutter.repo_name}} \\\n--pvc &lt;NAME_OF_DATA_SOURCE&gt;:/&lt;NAME_OF_DATA_SOURCE&gt; \\\n--cpu 2 \\\n--memory 4G \\\n-e AWS_ACCESS_KEY_ID=SECRET:s3-credentials,accessKeyId \\\n-e AWS_SECRET_ACCESS_KEY=SECRET:s3-credentials,secretAccessKey \\\n-e MLFLOW_S3_ENDPOINT_URL=\"https://necs.nus.edu.sg\" \\\n-e MLFLOW_TRACKING_USERNAME=&lt;YOUR_MLFLOW_USERNAME&gt; \\\n-e MLFLOW_TRACKING_PASSWORD=&lt;YOUR_MLFLOW_PASSWORD&gt; \\\n-e MLFLOW_HPTUNING_TAG=$(date +%s) \\\n--command -- '/bin/bash -c \"source activate {{cookiecutter.repo_name}} &amp;&amp; python src/train_model.py --multirun train_model.data_dir_path=/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/data/processed/mnist-pngs-data-aisg-processed train_model.setup_mlflow=true train_model.mlflow_tracking_uri=&lt;MLFLOW_TRACKING_URI&gt; train_model.mlflow_exp_name=&lt;NAME_OF_DEFAULT_MLFLOW_EXPERIMENT&gt; train_model.model_checkpoint_dir_path=/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/{{cookiecutter.repo_name}}/models train_model.epochs=3\"'\n</code></pre> <pre><code>$ runai submit `\n    --job-name-prefix &lt;YOUR_HYPHENATED_NAME&gt;-train `\n    -i {{cookiecutter.harbor_registry_project_path}}/model-training:0.1.0 `\n    --working-dir /home/aisg/{{cookiecutter.repo_name}} `\n    --pvc &lt;NAME_OF_DATA_SOURCE&gt;:/&lt;NAME_OF_DATA_SOURCE&gt; `\n    --cpu 2 `\n    --memory 4G `\n    -e AWS_ACCESS_KEY_ID=SECRET:s3-credentials,accessKeyId `\n    -e AWS_SECRET_ACCESS_KEY=SECRET:s3-credentials,secretAccessKey `\n    -e MLFLOW_S3_ENDPOINT_URL=\"https://necs.nus.edu.sg\" `\n    -e MLFLOW_TRACKING_USERNAME=&lt;YOUR_MLFLOW_USERNAME&gt; `\n    -e MLFLOW_TRACKING_PASSWORD=&lt;YOUR_MLFLOW_PASSWORD&gt; `\n    -e MLFLOW_HPTUNING_TAG=$(Get-Date -UFormat %s -Millisecond 0) `\n    --command -- \"/bin/bash -c 'source activate {{cookiecutter.repo_name}} &amp;&amp; python src/train_model.py --multirun train_model.data_dir_path=/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/data/processed/mnist-pngs-data-aisg-processed train_model.setup_mlflow=true train_model.mlflow_tracking_uri=&lt;MLFLOW_TRACKING_URI&gt; train_model.mlflow_exp_name=&lt;NAME_OF_DEFAULT_MLFLOW_EXPERIMENT&gt; train_model.model_checkpoint_dir_path=/&lt;NAME_OF_DATA_SOURCE&gt;/workspaces/&lt;YOUR_HYPHENATED_NAME&gt;/{{cookiecutter.repo_name}}/models train_model.epochs=3'\"\n</code></pre> <p></p> <p>Reference(s):</p> <ul> <li>Run:ai Docs - Environment Variables inside a Run:ai Workload</li> <li>Hydra Docs - Optuna Sweeper Plugin</li> <li>MLflow Docs - Search Syntax</li> </ul>"},{"location":"guide-for-user/08-deployment/","title":"Deployment","text":"<p>Assuming we have a predictive model that we are satisfied with, we can serve it within a REST API service with which requests can be made to and predictions are returned.</p> <p>Python has plenty of web frameworks that we can leverage on to build our REST API. Popular examples include Flask, Django and Starlette. For this guide however, we will resort to the well-known FastAPI (which is based on Starlette itself).</p> <p>Reference(s):</p> <ul> <li>IBM Technology - What is a REST API? (Video)</li> </ul>"},{"location":"guide-for-user/08-deployment/#model-artifacts","title":"Model Artifacts","text":"<p>Seen in \"Model Training\" , we have the trained models uploaded to ECS through the MLflow Tracking server (done through autolog). With that, we have the following pointers to take note of:</p> <ul> <li>By default, each MLflow experiment run is given a unique ID.</li> <li>When artifacts are uploaded to ECS through MLflow,   the artifacts are located within directories named after the   unique IDs of the runs.</li> <li>Artifacts for specific runs will be uploaded to a directory with a   convention similar to the following:   <code>&lt;MLFLOW_EXPERIMENT_ARTIFACT_LOCATION&gt;/&lt;MLFLOW_RUN_UUID&gt;/artifacts</code>.</li> <li>With this path/URI, we can use the AWS CLI to download the predictive   model from ECS into a mounted volume when we run the Docker image for   the REST APIs.</li> </ul> <p>Here's how you can quickly retrieve the artifact location of a specific MLflow experiment within your VSCode server:</p> Linux/macOSWindows PowerShell <pre><code>$ export MLFLOW_TRACKING_URI=&lt;MLFLOW_TRACKING_URI&gt;\n$ export MLFLOW_TRACKING_USERNAME=&lt;MLFLOW_TRACKING_USERNAME&gt;\n$ export MLFLOW_TRACKING_PASSWORD=&lt;MLFLOW_TRACKING_PASSWORD&gt;\n$  python -c \"import mlflow; mlflow_experiment = mlflow.get_experiment_by_name('&lt;NAME_OF_DEFAULT_MLFLOW_EXPERIMENT&gt;'); print(mlflow_experiment.artifact_location)\"\ns3://&lt;BUCKET_NAME&gt;/subdir/paths\n</code></pre> <pre><code>$ $Env:MLFLOW_TRACKING_URI=&lt;MLFLOW_TRACKING_URI&gt;\n$ $Env:MLFLOW_TRACKING_USERNAME=&lt;MLFLOW_TRACKING_USERNAME&gt;\n$ $Env:MLFLOW_TRACKING_PASSWORD=&lt;MLFLOW_TRACKING_PASSWORD&gt;\n$  python -c \"import mlflow; mlflow_experiment = mlflow.get_experiment_by_name('&lt;NAME_OF_DEFAULT_MLFLOW_EXPERIMENT&gt;'); print(mlflow_experiment.artifact_location)\"\ns3://&lt;BUCKET_NAME&gt;/subdir/paths\n</code></pre> <p>To list the contents of the artifact location, you can use the AWS CLI (installed within the VSCOde server by default) like so:</p> Linux/macOSWindows PowerShell <pre><code>$ export AWS_ACCESS_KEY_ID=&lt;AWS_ACCESS_KEY_ID&gt;\n$ export AWS_SECRET_ACCESS_KEY=&lt;AWS_SECRET_ACCESS_KEY&gt;\n$ aws s3 ls --endpoint-url \"https://necs.nus.edu.sg\" &lt;MLFLOW_EXPERIMENT_ARTIFACT_LOCATION&gt;/\n                       PRE XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX/\n                       PRE YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY/\n                       PRE ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ/\n</code></pre> <pre><code>$ $Env:AWS_ACCESS_KEY_ID=&lt;AWS_ACCESS_KEY_ID&gt;\n$ $Env:AWS_SECRET_ACCESS_KEY=&lt;AWS_SECRET_ACCESS_KEY&gt;\n$ aws s3 ls --endpoint-url \"https://necs.nus.edu.sg\" &lt;MLFLOW_EXPERIMENT_ARTIFACT_LOCATION&gt;/\n                       PRE XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX/\n                       PRE YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY/\n                       PRE ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ/\n</code></pre> <p>What would be listed are subdirectories named after MLflow experiment unique IDs. Within each of these subdirectories, we will find the artifacts uploaded to ECS.To list the artifacts of a specific run, we can run a command like the following:</p> Linux/macOSWindows PowerShell <pre><code>$ aws s3 ls --recursive --endpoint-url \"https://necs.nus.edu.sg\" &lt;MLFLOW_EXPERIMENT_ARTIFACT_LOCATION&gt;/&lt;MLFLOW_RUN_UUID&gt;\nYYYY-MM-DD hh:mm:ss   XXXXXXXX &lt;MLFLOW_EXPERIMENT_ARTIFACT_LOCATION&gt;/&lt;MLFLOW_RUN_UUID&gt;/artifacts/model/model.pt\nYYYY-MM-DD hh:mm:ss   XXXXXXXX &lt;MLFLOW_EXPERIMENT_ARTIFACT_LOCATION&gt;/&lt;MLFLOW_RUN_UUID&gt;/artifacts/train_model_config.json\n</code></pre> <pre><code>$ aws s3 ls --recursive --endpoint-url \"https://necs.nus.edu.sg\" &lt;MLFLOW_EXPERIMENT_ARTIFACT_LOCATION&gt;/&lt;MLFLOW_RUN_UUID&gt;\nYYYY-MM-DD hh:mm:ss   XXXXXXXX &lt;MLFLOW_EXPERIMENT_ARTIFACT_LOCATION&gt;/&lt;MLFLOW_RUN_UUID&gt;/artifacts/model/model.pt\nYYYY-MM-DD hh:mm:ss   XXXXXXXX &lt;MLFLOW_EXPERIMENT_ARTIFACT_LOCATION&gt;/&lt;MLFLOW_RUN_UUID&gt;/artifacts/train_model_config.json\n</code></pre> <p>Now that we have established on how we are to obtain the models for the API server, let's look into the servers themselves.</p>"},{"location":"guide-for-user/08-deployment/#model-serving-fastapi","title":"Model Serving (FastAPI)","text":"<p>FastAPI is a web framework that has garnered much popularity in recent years due to ease of adoption with its comprehensive tutorials, type and schema validation, being async capable and having automated docs, among other things. These factors have made it a popular framework within AI Singapore across many projects.</p> <p>If you were to inspect the <code>src</code> folder, you would notice that there exist more than one package:</p> <ul> <li><code>{{cookiecutter.src_package_name}}</code></li> <li><code>{{cookiecutter.src_package_name}}_fastapi</code></li> </ul> <p>The former contains the modules for executing pipelines like data preparation and model training while the latter is dedicated to modules meant for the REST API. Regardless, the packages can be imported by each other.</p> <p>Note</p> <p>It is recommended that you grasp some basics of the FastAPI framework, up till the beginner tutorials for better understanding of this section.</p> <p>Let's try running the boilerplate API server on a local machine. Before doing that, identify from the MLflow dashboard the unique ID of the experiment run that resulted in the predictive model that you would like to serve.</p> <p></p> <p>With reference to the example screenshot above, the UUID for the experiment run is <code>7251ac3655934299aad4cfebf5ffddbe</code>. Once the ID of the MLflow run has been obtained, let's download the model that we intend to serve. Assuming you're in the root of this template's repository, execute the following commands:</p> Linux/macOSWindows PowerShell <pre><code>$ export PRED_MODEL_UUID=&lt;MLFLOW_RUN_UUID&gt;\n$ export PRED_MODEL_ECS_S3_URI=&lt;MLFLOW_EXPERIMENT_ARTIFACT_LOCATION&gt;/$PRED_MODEL_UUID\n$ aws s3 cp --recursive --endpoint-url \"https://necs.nus.edu.sg\" $PRED_MODEL_ECS_S3_URI ./models/$PRED_MODEL_UUID\n</code></pre> <pre><code>$ $Env:PRED_MODEL_UUID=&lt;MLFLOW_RUN_UUID&gt;\n$ $Env:PRED_MODEL_ECS_S3_URI=&lt;MLFLOW_EXPERIMENT_ARTIFACT_LOCATION&gt;/$Env:MLFLOW_RUN_UUID\n$ aws s3 cp --recursive --endpoint-url \"https://necs.nus.edu.sg\" $Env:PRED_MODEL_ECS_S3_URI .\\models\\$Env:PRED_MODEL_UUID\n</code></pre> <p>Executing the commands above will download the artifacts related to the experiment run <code>&lt;MLFLOW_RUN_UUID&gt;</code> to this repository's subdirectory <code>models</code>. However, the specific subdirectory that is relevant for our modules to load will be <code>./models/&lt;MLFLOW_RUN_UUID&gt;/artifacts/model/model.pt</code>. Let's export this path to an environment variable:</p> Linux/macOSWindows PowerShell <pre><code>$ export PRED_MODEL_PATH=\"$PWD/models/$PRED_MODEL_UUID/artifacts/model/model.pt\"\n</code></pre> <pre><code>$ $Env:PRED_MODEL_PATH=\"$(Get-Location)\\models\\$Env:PRED_MODEL_UUID\\artifacts\\model\\model.pt\"\n</code></pre> <p>The variable exported above (<code>PRED_MODEL_UUID</code> and <code>PRED_MODEL_PATH</code>) will be used by the FastAPI server to load the model for prediction. We will get back to this in a bit. For now, let's proceed and spin up an inference server using the package that exists within the repository.</p>"},{"location":"guide-for-user/08-deployment/#local-server","title":"Local Server","text":"<p>Run the FastAPI server using Gunicorn (for Linux/macOS) or <code>uvicorn</code> (for Windows):</p> <p>Attention</p> <p>Gunicorn is only executable on UNIX-based or UNIX-like systems, this method would not be possible/applicable for Windows machine.</p> Linux/macOSWindows PowerShell <pre><code>$ conda activate {{cookiecutter.repo_name}}\n$ cd src\n$ gunicorn {{cookiecutter.src_package_name}}_fastapi.main:APP -b 0.0.0.0:8080 -w 4 -k uvicorn.workers.UvicornWorker\n</code></pre> <p>See here as to why Gunicorn is to be used instead of just Uvicorn. TLDR: Gunicorn is needed to spin up multiple processes/workers to handle more requests i.e. better for the sake of production needs.</p> <pre><code>$ conda activate {{cookiecutter.repo_name}}\n$ cd src\n$ uvicorn {{cookiecutter.src_package_name}}_fastapi.main:APP\n</code></pre> <p>In another terminal, use the <code>curl</code> command to submit a request to the API:</p> Linux/macOSWindows PowerShell <pre><code>$ curl -X POST \\\nlocalhost:8080/api/v1/model/predict \\\n-H 'Content-Type: multipart/form-data' \\\n-F \"image_file=@/path/to/image/file\"\n{\"data\":[{\"image_filename\":\"XXXXX.png\",\"prediction\":\"X\"}]}\n</code></pre> <pre><code>$ curl.exe '-X', 'POST', `\n'localhost:8080/api/v1/model/predict', `\n'-H', 'Content-Type: multipart/form-data', `\n'-F', '\"image_file=@\\path\\to\\image\\file\"',\n{\"data\":[{\"image_filename\":\"XXXXX.png\",\"prediction\":\"X\"}]}\n</code></pre> <p>With the returned JSON object, we have successfully submitted a request to the FastAPI server and it returned predictions as part of the response.</p>"},{"location":"guide-for-user/08-deployment/#pydantic-settings","title":"Pydantic Settings","text":"<p>Now you might be wondering, how does the FastAPI server knows the path to the model for it to load? FastAPI utilises Pydantic, a library for data and schema validation, as well as settings management. There's a class called <code>Settings</code> under the module <code>src/{{cookiecutter.src_package_name}}_fastapi/config.py</code>. This class contains several fields: some are defined and some others not. The fields <code>PRED_MODEL_UUID</code> and <code>PRED_MODEL_PATH</code> inherit their values from the environment variables.</p> <p><code>src/{{cookiecutter.src_package_name}}_fastapi/config.py</code>:</p> <pre><code>...\nclass Settings(pydantic.BaseSettings):\n\n    API_NAME: str = \"{{cookiecutter.src_package_name}}_fastapi\"\n    API_V1_STR: str = \"/api/v1\"\n    LOGGER_CONFIG_PATH: str = \"../conf/base/logging.yaml\"\n\n    USE_CUDA: bool = False\n    USE_MPS: bool = False\n    PRED_MODEL_UUID: str\n    PRED_MODEL_PATH: str\n...\n</code></pre> <p>FastAPI automatically generates interactive API documentation for easy viewing of all the routers/endpoints you have made available for the server. You can view the documentation through <code>&lt;API_SERVER_URL&gt;:&lt;PORT&gt;/docs</code>. In our case here, it is viewable through <code>localhost:8080/docs</code>. It will look like such:</p> <p></p> <p>Reference(s):</p> <ul> <li>PyTorch Tutorials - Saving and Loading Models</li> <li>FastAPI Docs</li> <li>Pydantic Docs - Settings Management</li> <li><code>curl</code> tutorial</li> </ul>"},{"location":"guide-for-user/09-batch-inferencing/","title":"Batch Inferencing","text":"<p>Some problem statements do not warrant the deployment of an API server but instead methods for conducting batched inferencing where a batch of data is provided to a script and it is able to churn out a set of predictions, perhaps exported to a file.</p> <p>This template provides a Python script (<code>src/batch_inferencing.py</code>) for this purpose.</p> <p>Let's first download some data on our local machine for us to conduct batch inferencing on:</p> Linux/macOSWindows PowerShell <pre><code>$ cd data\n$ wget https://storage.googleapis.com/aisg-mlops-pub-data/kapitan-hull/batched-mnist-input-data.zip\n$ unzip batched-mnist-input-data.zip\n$ rm batched-mnist-input-data.zip\n</code></pre> <pre><code>$ cd data\n$ wget https://storage.googleapis.com/aisg-mlops-pub-data/kapitan-hull/batched-mnist-input-data.zip\n$ Expand-Archive -Path batched-mnist-input-data.zip -DestinationPath .\n$ rm batched-mnist-input-data.zip\n</code></pre> <p>To execute the batch inferencing script locally:</p> Linux/macOSWindows PowerShell <pre><code># Navigate back to root directory\n$ cd \"$(git rev-parse --show-toplevel)\"\n$ export PRED_MODEL_UUID=&lt;MLFLOW_RUN_UUID&gt;\n$ export PRED_MODEL_PATH=\"$PWD/models/$PRED_MODEL_UUID/artifacts/model/model.pt\"\n$ conda activate {{cookiecutter.repo_name}}\n$ python src/batch_inferencing.py \\\nbatch_infer.model_path=$PRED_MODEL_PATH \\\nbatch_infer.input_data_dir=\"$PWD/data/batched-mnist-input-data\"\n</code></pre> <pre><code># Navigate back to root directory\n$ cd \"$(git rev-parse --show-toplevel)\"\n$ $Env:PRED_MODEL_UUID=&lt;MLFLOW_RUN_UUID&gt;\n$ $Env:PRED_MODEL_PATH=\"$(Get-Location)\\models\\$PRED_MODEL_UUID\\artifacts\\model\\model.pt\"\n$ conda activate {{cookiecutter.repo_name}}\n$ python src/batch_inferencing.py `\n    hydra.job.chdir=True `\n    batch_infer.model_path=$Env:PRED_MODEL_PATH `\n    batch_infer.input_data_dir=\"$(Get-Location)\\data\\batched-mnist-input-data\"\n</code></pre> <p>The script will log to the terminal the location of the <code>.jsonl</code> file (<code>batch-infer-res.jsonl</code>) containing predictions that look like such:</p> <pre><code>...\n{\"time\": \"YYYY-MM-DDThh:mm:ss+0000\", \"image_filepath\": \"/home/aisg/{{cookiecutter.repo_name}}/data/XXXXXX.png\", \"prediction\": \"X\"}\n{\"time\": \"YYYY-MM-DDThh:mm:ss+0000\", \"image_filepath\": \"/home/aisg/{{cookiecutter.repo_name}}/data/XXXXXX.png\", \"prediction\": \"X\"}\n{\"time\": \"YYYY-MM-DDThh:mm:ss+0000\", \"image_filepath\": \"/home/aisg/{{cookiecutter.repo_name}}/data/XXXXXX.png\", \"prediction\": \"X\"}\n{\"time\": \"YYYY-MM-DDThh:mm:ss+0000\", \"image_filepath\": \"/home/aisg/{{cookiecutter.repo_name}}/data/XXXXXX.png\", \"prediction\": \"X\"}\n{\"time\": \"YYYY-MM-DDThh:mm:ss+0000\", \"image_filepath\": \"/home/aisg/{{cookiecutter.repo_name}}/data/XXXXXX.png\", \"prediction\": \"X\"}\n...\n</code></pre> <p>The <code>hydra.job.chdir=True</code> flag writes the <code>.jsonl</code> file containing the predictions to a subdirectory within the <code>outputs</code> folder. See here for more information on outputs generated by Hydra.</p>"},{"location":"guide-for-user/10-cicd/","title":"Continuous Integration &amp; Deployment","text":"<p>This template presents users with a base configuration for a GitLab CI/CD pipeline. In this section, the guide aims to provide readers with some basic understanding of the pipeline defined in the configuration file <code>.gitlab-ci.yml</code>.</p> <p>That being said, readers would certainly benefit from reading up on introductory CI/CD concepts as introduced by GitLab's Docs.</p>"},{"location":"guide-for-user/10-cicd/#github-flow","title":"GitHub Flow","text":"<p>The defined pipeline assumes a GitHub flow which only relies on feature branches and a <code>main</code> (default) branch.</p> <p></p> <p>With reference to the diagram above, we have the following pointers:</p> <ul> <li>We make use of feature branches   (<code>git checkout -b &lt;NAME_OF_BRANCH&gt;</code>) to introduce changes to the   source.</li> <li>Merge requests are made when we intend to merge the commits made to a   feature branch to <code>main</code>.</li> <li>While one works on a feature branch, it is recommended that changes   pushed to the <code>main</code> are pulled to the feature branch itself on a   consistent basis. This allows the feature branch to possess the   latest changes pushed by other developers through their own feature   branches. In the example above, commits from the <code>main</code> branch   following a merge of the <code>add-hidden-layer</code> branch are pulled into   the <code>change-training-image</code> branch while that branch still expects   further changes.</li> <li>The command <code>git pull</code> can be used to pull and sync   these changes. However, it's recommended that developers make use of   <code>git fetch</code> and <code>git log</code> to observe incoming changes first rather   than pulling in changes in an indiscriminate manner.</li> <li>While it's possible for commits to be made directly   to the <code>main</code> branch, it's recommended that they are kept minimal,   at least for GitHub flow (other workflows might not heed such   practices).</li> </ul> <p>As we move along, we should be able to relate parts of the flow described above with the stages defined by the default GitLab CI pipeline.</p>"},{"location":"guide-for-user/10-cicd/#environment-variables","title":"Environment Variables","text":"<p>Before we can make use of the GitLab CI pipeline, we would have to define the following variable(s) for the pipeline beforehand:</p> <ul> <li><code>HARBOR_ROBOT_CREDS_JSON</code>: A JSON formatted value that contains   encoded credentials for a robot account on Harbor. This is to allow   the pipeline to interact with the Harbor server. See   here on how to   generate this value/file.</li> </ul> <p>To define CI/CD variables for a project (repository), follow the steps listed here. The environment variable <code>HARBOR_ROBOT_CREDS_JSON</code> needs to be a <code>File</code> type.</p>"},{"location":"guide-for-user/10-cicd/#docker-configuration-file-for-accessing-harbor","title":"Docker Configuration File for Accessing Harbor","text":"<p>The variable <code>HARBOR_ROBOT_CREDS_JSON</code> will be used to populate the files <code>/kaniko/.docker/config.json</code> and <code>/root/.docker/config.json</code> for <code>kaniko</code> and <code>crane</code> to authenticate themselves before communicating with AI Singapore's Harbor registry. You may create the JSON file like so:</p> Linux/macOSWindows PowerShell <pre><code>$ echo -n &lt;HARBOR_USERNAME&gt;:&lt;HARBOR_PASSWORD&gt; | base64\n&lt;ENCODED_OUTPUT_HERE&gt;\n</code></pre> <pre><code>$ $cred = \"&lt;HARBOR_USERNAME&gt;:&lt;HARBOR_PASSWORD&gt;\"\n$ $bytes = [System.Text.Encoding]::ASCII.GetBytes($cred)\n$ $base64 = [Convert]::ToBase64String($bytes)\n$ echo $base64\n&lt;ENCODED_OUTPUT_HERE&gt;\n</code></pre> <p>Using the output from above, copy and paste the following content into a CI/CD environment variable of type <code>File</code> (under <code>Settings</code> -&gt; <code>CI/CD</code> -&gt; <code>Variables</code> -&gt; <code>Add variable</code>):</p> <pre><code>{\n\"auths\": {\n\"registry.aisingapore.net\": {\n\"auth\": \"&lt;ENCODED_OUTPUT_HERE&gt;\"\n}\n}\n}\n</code></pre> <p></p> <p>Reference(s):</p> <ul> <li>GitLab Docs - GitLab CI/CD variables</li> <li>Docker Docs - Configuration files</li> </ul>"},{"location":"guide-for-user/10-cicd/#stages-jobs","title":"Stages &amp; Jobs","text":"<p>In the default pipeline, we have 3 stages defined:</p> <ul> <li><code>test</code>: For every push to certain branches, the source code residing   in <code>src</code> will be tested.</li> <li><code>build</code>: Assuming the automated tests are passed, the pipeline   will build Docker images, making use of the latest source.</li> <li><code>deploy-docs</code>: This stage is for the purpose of deploying a static   site through   GitLab Pages.   More on this stage is covered in   \"Documentation\".</li> </ul> <p>These stages are defined and listed like so:</p> <code>.gitlab-ci.yml</code> <pre><code>...\nstages:\n- test\n- build\n- deploy-docs\n...\n</code></pre> <p>The jobs for each of the stages are executed using Docker images defined by users. For this, we have to specify in the pipeline the tag associated with the GitLab Runner that has the Docker executor. In our case, the tag for the relevant runner is <code>dind</code>. The <code>on-prem</code> tag calls for runners within our on-premise infrastructure so on-premise services can be accessed within our pipelines.</p> <code>.gitlab-ci.yml</code> <pre><code>default:\ntags:\n- dind\n- on-prem\n...\n</code></pre>"},{"location":"guide-for-user/10-cicd/#automated-testing-linting","title":"Automated Testing &amp; Linting","text":"<p>Let's look at the job defined for the <code>test</code>stage first:</p> <code>.gitlab-ci.yml</code> <pre><code>...\ntest:pylint-pytest:\nstage: test\nimage:\nname: continuumio/miniconda:4.7.12\nbefore_script:\n- conda env create -f {{cookiecutter.repo_name}}-conda-env.yaml\n- source activate {{cookiecutter.repo_name}}\nscript:\n- pylint src --fail-under=7.0 --ignore=tests --disable=W1202\n- pytest src/tests\nrules:\n- if: $CI_MERGE_REQUEST_IID\nchanges:\n- src/**/*\n- conf/**/*\n- if: $CI_PIPELINE_SOURCE == \"push\"\n- if: $CI_COMMIT_TAG\nwhen: never\n...\n</code></pre> <p>First of all, this <code>test:pylint-pytest</code> job will only execute on the condition that the defined <code>rules</code> are met. In this case, the job will only execute for the following cases:</p> <ul> <li>For any pushes to any branch.</li> <li>For pushes to branches which merge requests have been created,   tests are executed only if there are changes made to any files within   <code>src</code> or <code>conf</code> are detected. This is to prevent automated tests   from running for pushes made to feature branches   with merge requests when no   changes have been made to files for which tests are relevant.   Otherwise, tests will run in a redundant manner, slowing down the   feedback loop.</li> <li>If the push action is associated with a tag   (<code>git push &lt;remote&gt; &lt;tag_name&gt;</code>), the job will not run.</li> </ul> <p>The job defined above fails under any of the following conditions:</p> <ul> <li>The source code does not meet a linting score of at least 7.0.</li> <li>The source code fails whatever tests have been defined under   <code>src/tests</code>.</li> </ul> <p>The job would have to succeed before moving on to the <code>build</code> stage. Otherwise, no Docker images will be built. This is so that source code that fail tests would never be packaged.</p> <p>Reference(s):</p> <ul> <li>GitLab Docs - Predefined variables reference</li> <li>Real Python - Effective Python Testing With Pytest</li> <li>VSCode Docs - Linting Python in Visual Studio Code</li> </ul>"},{"location":"guide-for-user/10-cicd/#automated-builds","title":"Automated Builds","text":"<p>The template has thus far introduced a couple of Docker images relevant for the team. The tags for all the Docker images are listed below:</p> <ul> <li><code>{{cookiecutter.harbor_registry_project_path}}/data-prep</code></li> <li><code>{{cookiecutter.harbor_registry_project_path}}/model-training</code></li> </ul> <p>The <code>build</code> stage aims at automating the building of these Docker images in a parallel manner. Let's look at a snippet for a single job that builds a Docker image:</p> <code>.gitlab-ci.yml</code> <pre><code>...\nbuild:data-prep-image:\nstage: build\nimage:\nname: gcr.io/kaniko-project/executor:debug\nentrypoint: [\"\"]\nscript:\n- mkdir -p /kaniko/.docker\n- cat $HARBOR_ROBOT_CREDS_JSON &gt; /kaniko/.docker/config.json\n- &gt;-\n/kaniko/executor\n--context \"${CI_PROJECT_DIR}\"\n--dockerfile \"${CI_PROJECT_DIR}/docker/{{cookiecutter.repo_name}}-data-prep.Dockerfile\"\n--destination \"{{cookiecutter.harbor_registry_project_path}}/data-prep:${CI_COMMIT_SHORT_SHA}\"\nrules:\n- if: $CI_MERGE_REQUEST_IID\nchanges:\n- docker/{{cookiecutter.repo_name}}-data-prep.Dockerfile\n- src/**/*\n- conf/**/*\n- if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH\n...\n</code></pre> <p>Note</p> <p>You would have noticed that the jobs for building images utilise the command <code>/kaniko/executor</code> as opposed to <code>docker build</code> which most users would be more familiar with. This is due to the usage of <code>kaniko</code> within a runner with a Docker executor. Using Docker within Docker (Docker-in-Docker) requires privileged mode that poses several security concerns. Hence, the image <code>gcr.io/kaniko-project/executor:debug</code> is being used for all <code>build</code> jobs related to building of Docker images. That being said, the flags used for <code>kaniko</code> corresponds well with the flags usually used for <code>docker</code> commands.</p> <p>Just like with the <code>test</code> job, the each of the jobs under <code>build</code> will execute under certain conditions:</p> <ul> <li>If a push is being done to a branch which has a merge request opened,   a check would be done to see if any changes were made to folders like   <code>src</code>, <code>conf</code>, <code>scripts</code>, or the relevant Dockerfile itself. If there   are changes, the job will be executed. An opened merge request is   detected through the predefined variable <code>CI_MERGE_REQUEST_IID</code>.</li> <li>If a push is being made to the default branch (<code>CI_DEFAULT_BRANCH</code>)   of the repo, which in   most cases within our organisation would be <code>main</code>, the job would   execute as well. Recalling the <code>test</code> stage, any pushes to the repo   would trigger the automated tests and linting. If a push to the   <code>main</code> branch passes the tests, all Docker images will be   built, regardless of whether changes have been made to files   relevant to the Docker images to be built themselves.</li> </ul> <p>Images built through the pipeline will be tagged with the commit hashes associated with the commits that triggered it. This is seen through the usage of the predefined variable <code>CI_COMMIT_SHORT_SHA</code>.</p> <p>Reference(s):</p> <ul> <li>GitLab Docs - Use kaniko to build Docker images</li> <li>GitLab Docs - Use Docker to build Docker images</li> </ul>"},{"location":"guide-for-user/10-cicd/#tagging","title":"Tagging","text":"<p>As mentioned, pushes to the default branch would trigger builds for Docker images and they would be tagged with the commit hash. However, such commit hashes aren't the best way to tag \"finalised\" Docker images so the usage of tags would be more appropriate here. Hence, for the job defined below, it would only trigger if a tag is pushed to the default branch and only the default branch. The tag pushed (say through a command like <code>git push &lt;remote&gt; &lt;tag&gt;</code>) to the default branch on the remote would have the runner retag the Docker image that exists on Harbor with the tag that is being pushed. The relevant images to be retagged are originally tagged with the short commit hash obtained from the commit that was pushed to the default branch before this.</p> <code>.gitlab-ci.yml</code> <pre><code>...\nbuild:retag-images:\nstage: build\nimage:\nname: gcr.io/go-containerregistry/crane:debug\nentrypoint: [\"\"]\nscript:\n- cat $HARBOR_ROBOT_CREDS_JSON &gt; /root/.docker/config.json\n- crane auth login registry.aisingapore.net\n- crane tag {{cookiecutter.harbor_registry_project_path}}/data-prep:${CI_COMMIT_SHORT_SHA} ${$CI_COMMIT_TAG}\n- crane tag {{cookiecutter.harbor_registry_project_path}}/model-training:${CI_COMMIT_SHORT_SHA} ${$CI_COMMIT_TAG}\nrules:\n- if: $CI_COMMIT_TAG &amp;&amp; $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH\n...\n</code></pre> <p>Reference(S):</p> <ul> <li>GitHub Docs - GitHub Flow</li> <li>GitLab Docs - GitLab Flow</li> <li><code>go-containerregistry</code> GitHub - <code>crane</code></li> </ul>"},{"location":"guide-for-user/10-cicd/#conclusion","title":"Conclusion","text":"<p>The stages and jobs defined in this default pipeline is rudimentary at best as there is much more that could be done with GitLab CI. Some examples off the top:</p> <ul> <li>automatically generate reports for datasets that arrive in regular   intervals</li> <li>submit model training jobs following triggers invoked by the same   pipeline</li> <li>automate the deployment of the FastAPI servers to Kubernetes clusters</li> </ul> <p>There's much more that can be done but whatever has been shared thus far is hopefully enough for one to get started with CI/CD.</p>"},{"location":"guide-for-user/11-documentation/","title":"Documentation","text":"<p>The boilerplate packages generated by the template are populated with some NumPy formatted docstrings. What we can do with this is to observe how documentation can be automatically generated using Sphinx, with the aid of the Napoleon extension. Let's build the HTML asset for the documentation:</p> <pre><code># From the root folder\n$ conda activate {{cookiecutter.repo_name}}\n$ sphinx-apidoc -f -o docs src\n$ sphinx-build -b html docs public\n</code></pre> <p>Open the file <code>public/index.html</code> with your browser and you will be presented with a static site similar to the one shown below:</p> <p></p> <p>Browse through the site and inspect the documentation that was automatically generated through Sphinx.</p>"},{"location":"guide-for-user/11-documentation/#gitlab-pages","title":"GitLab Pages","text":"<p>Documentation generated through Sphinx can be served on GitLab Pages, through GitLab CI/CD. With this template, a default CI job has been defined in <code>.gitlab-ci.yml</code> to serve the Sphinx documentation when pushes are done to the <code>main</code> branch:</p> <pre><code>...\npages:\nstage: deploy-docs\nimage:\nname: continuumio/miniconda:4.7.12\nscript:\n- conda env update -f {{cookiecutter.repo_name}}-conda-env.yaml\n- conda init bash\n- source ~/.bashrc\n- conda activate {{cookiecutter.repo_name}}\n- sphinx-apidoc -f -o docs src\n- sphinx-build -b html docs public\nartifacts:\npaths:\n- public\nonly:\n- main\n...\n</code></pre> <p>The documentation page is viewable through the following convention: <code>&lt;NAMESPACE&gt;.gitlab.aisingapore.net/&lt;PROJECT_NAME&gt;</code> or <code>&lt;NAMESPACE&gt;.gitlab.aisingapore.net/&lt;GROUP&gt;/&lt;PROJECT_NAME&gt;</code>.</p> <p>Reference(s):</p> <ul> <li>GitLab Docs - Pages domain names, URLs, and base URLs</li> <li>GitLab Docs - Namespaces</li> </ul>"}]}